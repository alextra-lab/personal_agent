# Personal Agent Configuration Template
# Copy this file to .env and customize for your local environment
# See: docs/architecture_decisions/ADR-0007-unified-configuration-management.md

# =============================================================================
# ENVIRONMENT
# =============================================================================
# Current environment: development, staging, production, test
# Default: development
# APP_ENV=development

# Debug mode (enables verbose logging and additional checks)
# Default: false
# APP_DEBUG=false

# =============================================================================
# APPLICATION
# =============================================================================
# Project name (informational)
# Default: Personal Local AI Collaborator
# PROJECT_NAME=Personal Local AI Collaborator

# Application version (informational)
# Default: 0.1.0
# VERSION=0.1.0

# =============================================================================
# TELEMETRY & LOGGING
# =============================================================================
# Log directory path (relative to project root or absolute)
# Default: telemetry/logs
# LOG_DIR=telemetry/logs

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# Default: INFO
# APP_LOG_LEVEL=INFO

# Log format: json (structured) or console (human-readable)
# Default: json
# APP_LOG_FORMAT=json

# =============================================================================
# LLM CLIENT
# =============================================================================
# Base URL for LLM API (OpenAI-compatible endpoint)
# Default: http://localhost:1234/v1 (LM Studio)
# Common alternatives:
#   - LM Studio: http://localhost:1234/v1
#   - Ollama: http://localhost:11434/v1
#   - OpenAI: https://api.openai.com/v1
LLM_BASE_URL=http://localhost:1234/v1

# LLM request timeout in seconds (1-600)
# Default: 120
# LLM_TIMEOUT_SECONDS=120

# Maximum retry attempts for failed LLM requests (0+)
# Default: 3
# LLM_MAX_RETRIES=3

# Optional suffix token to discourage verbose reasoning (model-specific)
# Works with some models (e.g., Qwen3) to reduce latency
# Default: /no_think
# LLM_NO_THINK_SUFFIX=/no_think

# Append no-think suffix to tool prompts (reduces latency)
# Default: true
# LLM_APPEND_NO_THINK_TO_TOOL_PROMPTS=true

# =============================================================================
# ORCHESTRATOR
# =============================================================================
# Maximum concurrent tasks (0+)
# Default: 5
# ORCHESTRATOR_MAX_CONCURRENT_TASKS=5

# Task timeout in seconds (1+)
# Default: 300
# ORCHESTRATOR_TASK_TIMEOUT_SECONDS=300

# Maximum tool execution iterations per user request (prevents loops)
# Default: 3
# ORCHESTRATOR_MAX_TOOL_ITERATIONS=3

# Maximum times the same tool call signature can repeat per request
# Default: 1
# ORCHESTRATOR_MAX_REPEATED_TOOL_CALLS=1

# =============================================================================
# BRAINSTEM (MODE MANAGEMENT)
# =============================================================================
# Sensor polling interval in seconds (>0)
# Default: 5.0
# BRAINSTEM_SENSOR_POLL_INTERVAL_SECONDS=5.0

# =============================================================================
# REQUEST MONITORING (ADR-0012)
# =============================================================================
# Enable automatic request-scoped metrics monitoring
# Default: true
# REQUEST_MONITORING_ENABLED=true

# Request monitoring polling interval in seconds (>0)
# Default: 5.0
# REQUEST_MONITORING_INTERVAL_SECONDS=5.0

# Include GPU metrics in request monitoring (Apple Silicon: powermetrics)
# Default: true
# REQUEST_MONITORING_INCLUDE_GPU=true

# =============================================================================
# MCP GATEWAY (ADR-0011)
# =============================================================================
# Enable Docker MCP Gateway integration
# Default: false
# AGENT_MCP_GATEWAY_ENABLED=false

# Command to run Docker MCP Gateway
# Can be JSON array: ["docker", "mcp", "gateway", "run"]
# Or space-separated: "docker mcp gateway run"
# Default: ["docker", "mcp", "gateway", "run"]
# AGENT_MCP_GATEWAY_COMMAND=docker mcp gateway run

# Timeout for MCP operations in seconds (1-300)
# Default: 60
# AGENT_MCP_GATEWAY_TIMEOUT_SECONDS=60

# List of MCP server names to enable (comma-separated, empty = all)
# Default: [] (all servers enabled)
# Example: server1,server2,server3
# AGENT_MCP_GATEWAY_ENABLED_SERVERS=

# =============================================================================
# CONFIGURATION PATHS
# =============================================================================
# Path to governance config directory (relative to project root or absolute)
# Default: config/governance
# GOVERNANCE_CONFIG_PATH=config/governance

# Path to model config file (relative to project root or absolute)
# Default: config/models.yaml
# MODEL_CONFIG_PATH=config/models.yaml

# =============================================================================
# SECRETS (NEVER COMMIT THESE VALUES)
# =============================================================================
# Add any API keys, tokens, or secrets here
# These should ONLY be in .env files (gitignored), never in code or YAML

# OpenAI API key (if using OpenAI instead of local LLM)
# OPENAI_API_KEY=sk-...

# Anthropic API key (if using Claude - Phase 2.2)
# ANTHROPIC_API_KEY=sk-ant-...

# Other service API keys as needed
# SERVICE_API_KEY=...

# =============================================================================
# PHASE 2.2: MEMORY & SECOND BRAIN
# =============================================================================
# Enable Neo4j memory graph
# Default: false
# AGENT_ENABLE_MEMORY_GRAPH=false

# Enable Second Brain background consolidation
# Default: false
# AGENT_ENABLE_SECOND_BRAIN=false

# Neo4j connection settings
# Default: bolt://localhost:7687
# AGENT_NEO4J_URI=bolt://localhost:7687
# AGENT_NEO4J_USER=neo4j
# AGENT_NEO4J_PASSWORD=neo4j_dev_password

# Entity extraction model for memory graph
# Options: 'lfm2.5-1.2b' (fast, default), 'qwen3-8b' (quality), 'claude' (cloud)
# Default: lfm2.5-1.2b
# AGENT_ENTITY_EXTRACTION_MODEL=lfm2.5-1.2b

# Claude API settings (optional, for production quality extraction)
# AGENT_CLAUDE_MODEL=claude-sonnet-4-5-20250514
# AGENT_CLAUDE_MAX_TOKENS=4096
# AGENT_CLAUDE_WEEKLY_BUDGET_USD=5.0

# Second Brain scheduling thresholds
# AGENT_SECOND_BRAIN_IDLE_TIME_SECONDS=300         # 5 minutes idle
# AGENT_SECOND_BRAIN_CPU_THRESHOLD=50.0            # CPU usage < 50%
# AGENT_SECOND_BRAIN_MEMORY_THRESHOLD=70.0         # Memory usage < 70%
# AGENT_SECOND_BRAIN_CHECK_INTERVAL_SECONDS=60     # Check every minute
# AGENT_SECOND_BRAIN_MIN_INTERVAL_SECONDS=3600     # Min 1 hour between runs

# =============================================================================
# PHASE 2.1: SERVICE FOUNDATION
# =============================================================================
# Service host and port
# AGENT_SERVICE_HOST=0.0.0.0
# AGENT_SERVICE_PORT=9000

# PostgreSQL database URL
# AGENT_DATABASE_URL=postgresql+asyncpg://agent:agent_dev_password@localhost:5432/personal_agent
# AGENT_DATABASE_ECHO=false

# Elasticsearch for structured logging
# AGENT_ELASTICSEARCH_URL=http://localhost:9200
# AGENT_ELASTICSEARCH_INDEX_PREFIX=agent-logs

# Docker compose passwords (for docker-compose.yml)
# POSTGRES_PASSWORD=agent_dev_password
# NEO4J_PASSWORD=neo4j_dev_password
