# Model configuration for Local LLM Client
#
# This file defines model configurations for different roles (router, reasoning, coding).
# Each role maps to a model identifier and configuration.
#
# The client automatically tries /v1/responses first, then falls back to /v1/chat/completions
# if the responses endpoint is unavailable.
#
# Configuration options:
# - id: Model identifier (required)
# - endpoint: Base URL for this model (optional, defaults to settings.llm_base_url)
#   Examples:
#     - "http://localhost:1234/v1" (same provider, uses base_url if omitted)
#     - "http://localhost:8001/v1" (different provider/port)
#     - "http://ollama:11434/v1" (different provider entirely)
# - context_length, quantization, max_concurrency, default_timeout: Model-specific settings

models:
  router:
    #id: "liquid/lfm2.5-1.2b"
    id: "qwen/qwen3-1.7b"
    context_length: 32768
    quantization: "8bit"
    max_concurrency: 4
    default_timeout: 10  # seconds (1.7B model is fast, typical routing ~3-4s)
    temperature: 0.1
    supports_function_calling: false  # Qwen3 has native function calling support
    #endpoint: "http://localhost:8000/v1"  # Optional: override base URL for this model
    endpoint: "http://127.0.0.1:1234/v1"

  standard:
    # Default "workhorse" model: general answers + tool orchestration, but avoid long "thinking".
    # This role is intended to be a non-thinking / fast model (or future MoE) that can still use tools.
    id: "qwen/qwen3-4b-2507"
    context_length: 40960
    quantization: "8bit"
    max_concurrency: 2
    default_timeout: 45  # seconds
    temperature: 0.3
    supports_function_calling: true  # Test if MLX backend handles function calling
    #endpoint: "http://localhost:8000/v1"  # Optional: override base URL for this model
    endpoint: "http://127.0.0.1:1234/v1"

  reasoning:
    # BASELINE: Memory-optimized stack (ADR-0008)
    # 14B @ 8bit = 14-20GB VRAM (vs 80B @ 5bit = 50-60GB)
    id: "qwen/qwen3-8b"
    context_length: 32768  # Optimized from 128K for MVP
    quantization: "8bit"
    max_concurrency: 2  # Increased from 1 (smaller footprint enables parallelism)
    default_timeout: 90  # seconds (increased for entity extraction workload)
    temperature: 0.4
    supports_function_calling: true  # Use /v1/responses (native tools); /no_think supported via prompt suffix
    #endpoint: "http://localhost:8000/v1"  # Optional: override base URL for this model
    endpoint: "http://127.0.0.1:1234/v1"

  coding:
    #id: "qwen/qwen3-coder-30b"
    id: "mistralai/devstral-small-2-2512"
    context_length: 32768
    quantization: "8bit"
    max_concurrency: 2
    default_timeout: 45  # seconds
    temperature: 0.2
    supports_function_calling: false  # Use /v1/chat/completions (LM Studio /responses can be slow for some models)
    #endpoint: "http://localhost:8000/v1"  # Optional: override base URL for this model
    endpoint: "http://127.0.0.1:1234/v1"

  # EXPERIMENTAL: Alternative models for A/B testing (not loaded by default)
  reasoning_heavy:
    # For complex reasoning tasks requiring more capacity
    #id: "qwen/qwen3-30b-a3b"
    id: "qwen/qwen3-next-80b"
    context_length: 32768
    quantization: "4bit"
    max_concurrency: 1
    default_timeout: 90
    temperature: 0.35
    supports_function_calling: false  # Uses text-based tool calls (ADR-0008)
    #endpoint: "http://localhost:8000/v1"  # Optional: override base URL for this model
    endpoint: "http://127.0.0.1:1234/v1"

  reasoning_baseline:
    # Original baseline for comparison experiments
    id: "qwen/qwen3-30b-a3b"
    context_length: 128000
    quantization: "5bit"
    max_concurrency: 1
    default_timeout: 60
    temperature: 0.4
    #endpoint: "http://localhost:8000/v1"  # Optional: override base URL for this model

  coding_large_context:
    # For large codebase analysis (future evaluation; high-memory MoE model)
    id: "qwen/qwen3-coder-next"
    context_length: 128000
    quantization: "4bit"
    max_concurrency: 1
    default_timeout: 60
    temperature: 0.2
    #endpoint: "http://localhost:8000/v1"  # Optional: override base URL for this model
