# Phase 2.3 Plan: Homeostasis & Feedback Loop

**Date**: January 23, 2026
**Status**: ğŸ“‹ **PLANNING**
**Prerequisites**: âœ… Phase 2.2 Complete

---

## ğŸ¯ Phase Objectives

Build a **self-regulating system** with intelligent feedback loops and data lifecycle management:

1. **Telemetry Enhancement**: Index all digital exhaust for analytics
2. **Data Lifecycle Management**: Automated retention and cleanup
3. **Adaptive Thresholds**: Self-tuning based on usage patterns
4. **Feedback Loops**: Quality monitoring and improvement suggestions
5. **Proactive Insights**: Pattern detection and recommendations

---

## ğŸ“Š Current State Assessment

### Digital Exhaust Inventory

| Data Type | Location | Current Size | Retention | Indexed |
|-----------|----------|--------------|-----------|---------|
| **File Logs** | `telemetry/logs/*.jsonl` | ~500MB max | 5 backups | âŒ Local only |
| **Elasticsearch Logs** | ES cluster | Unknown | 30d default? | âœ… Yes |
| **Captain's Log Reflections** | `telemetry/captains_log/CL-*.json` | 145 files (~576KB) | â™¾ï¸ Forever | âŒ No |
| **Task Captures** | `telemetry/captains_log/captures/` | 145 files (~576KB) | â™¾ï¸ Forever | âŒ No |
| **Neo4j Knowledge Graph** | Neo4j DB | 84 nodes, 89 edges | â™¾ï¸ Forever | âœ… Yes |
| **PostgreSQL Cost Data** | Postgres DB | Minimal | â™¾ï¸ Forever | âœ… Yes |
| **Trace Data** | Embedded in logs | N/A | Same as logs | âš ï¸ Partial |

### Key Observations

**Problems:**
- âŒ No retention policies for file-based data
- âŒ Captain's Log data not searchable/queryable
- âŒ Unlimited growth for JSON files (will hit storage limits)
- âŒ No archival strategy
- âŒ No data volume monitoring
- âŒ Manual cleanup required

**Opportunities:**
- âœ… Elasticsearch infrastructure operational
- âœ… Structured data formats (easy to index)
- âœ… All data has timestamps (easy to age-based cleanup)
- âœ… Telemetry framework in place

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PHASE 2.3 ARCHITECTURE                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   1. TELEMETRY ENHANCEMENT                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Captain's Log â†’ Elasticsearch Integration                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚  â”‚ capture.py     â”‚â”€â”€write_capture()â”€â”€â”                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”œâ”€â”€â–¶ Elasticsearch        â”‚
â”‚  â”‚ manager.py     â”‚â”€â”€save_entry()â”€â”€â”€â”€â”€â”˜    (agent-captains-*)  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚                                                                  â”‚
â”‚  Benefits:                                                       â”‚
â”‚  â€¢ Real-time task analytics                                     â”‚
â”‚  â€¢ Searchable reflections                                       â”‚
â”‚  â€¢ Pattern detection                                            â”‚
â”‚  â€¢ Kibana dashboards                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              2. DATA LIFECYCLE MANAGEMENT                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Automated Retention & Cleanup                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ DataLifecycleManager                                    â”‚    â”‚
â”‚  â”‚  â€¢ Retention policies (configurable per data type)     â”‚    â”‚
â”‚  â”‚  â€¢ Archival (compress & move to cold storage)          â”‚    â”‚
â”‚  â”‚  â€¢ Deletion (purge expired data)                        â”‚    â”‚
â”‚  â”‚  â€¢ Monitoring (disk usage alerts)                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â”‚  Scheduled Tasks (via Brainstem):                               â”‚
â”‚  â€¢ Hourly: Check disk usage                                     â”‚
â”‚  â€¢ Daily: Archive old captures (>14d)                           â”‚
â”‚  â€¢ Weekly: Purge expired data (>90d)                            â”‚
â”‚  â€¢ Monthly: Elasticsearch index lifecycle management            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              3. ADAPTIVE THRESHOLD TUNING                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Self-Tuning Based on Historical Data                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ ThresholdOptimizer                                      â”‚    â”‚
â”‚  â”‚  â€¢ Analyze CPU/memory patterns (ES query)              â”‚    â”‚
â”‚  â”‚  â€¢ Detect false positives (mode transitions)           â”‚    â”‚
â”‚  â”‚  â€¢ Propose threshold adjustments                        â”‚    â”‚
â”‚  â”‚  â€¢ A/B test new thresholds                             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â”‚  Metrics Tracked:                                               â”‚
â”‚  â€¢ Mode transition frequency                                    â”‚
â”‚  â€¢ Resource utilization patterns                                â”‚
â”‚  â€¢ Consolidation trigger accuracy                               â”‚
â”‚  â€¢ User interruption rate                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              4. FEEDBACK LOOP FOR QUALITY                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Consolidation Quality Monitoring                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ ConsolidationFeedback                                   â”‚    â”‚
â”‚  â”‚  â€¢ Track entity extraction quality                      â”‚    â”‚
â”‚  â”‚  â€¢ Monitor graph growth rate                            â”‚    â”‚
â”‚  â”‚  â€¢ Detect duplicate entities                            â”‚    â”‚
â”‚  â”‚  â€¢ Measure relationship accuracy                        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â”‚  Quality Signals:                                               â”‚
â”‚  â€¢ Entity count vs conversation count ratio                     â”‚
â”‚  â€¢ Relationship density (edges per node)                        â”‚
â”‚  â€¢ Entity name similarity (detect duplicates)                   â”‚
â”‚  â€¢ User query success rate (found relevant memory?)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              5. PROACTIVE INSIGHTS ENGINE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Pattern Detection & Recommendations                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ InsightsEngine                                          â”‚    â”‚
â”‚  â”‚  â€¢ Analyze cross-data patterns (ES + Neo4j + Postgres) â”‚    â”‚
â”‚  â”‚  â€¢ Generate improvement suggestions                     â”‚    â”‚
â”‚  â”‚  â€¢ Detect anomalies                                     â”‚    â”‚
â”‚  â”‚  â€¢ Create Captain's Log proposals                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â”‚  Example Insights:                                              â”‚
â”‚  â€¢ "Tool X fails 60% when memory > 70%"                         â”‚
â”‚  â€¢ "Consolidation most effective at 3AM"                        â”‚
â”‚  â€¢ "Entity 'Python' mentioned in 40% of tasks"                  â”‚
â”‚  â€¢ "Cost spike detected: $2.50 today vs $0.50 avg"             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Implementation Checklist

### Part 1: Telemetry Enhancement (Days 1-2)

#### 1.1 Captain's Log â†’ Elasticsearch Integration

**Files to Modify:**
- [ ] `src/personal_agent/captains_log/capture.py`
  - Add optional ES handler parameter to `write_capture()`
  - Send capture to ES index `agent-captains-captures-YYYY-MM-DD`
  - Non-blocking (don't fail if ES down)

- [ ] `src/personal_agent/captains_log/manager.py`
  - Add optional ES handler parameter to `save_entry()`
  - Send reflection to ES index `agent-captains-reflections-YYYY-MM-DD`
  - Include full reflection with structured fields

- [ ] `src/personal_agent/service/app.py`
  - Pass ES handler to CaptainLogManager during lifespan
  - Configure index naming patterns

**Testing:**
- [ ] Test capture streaming to ES
- [ ] Test reflection streaming to ES
- [ ] Test ES connection failure handling (non-blocking)
- [ ] Verify index structure in Kibana

#### 1.2 Kibana Dashboards

**Dashboards to Create:**
- [ ] **Task Analytics Dashboard**
  - Task outcome distribution (completed/failed/timeout)
  - Average duration by tool usage
  - Most frequent tools
  - Memory context usage rate

- [ ] **Reflection Insights Dashboard**
  - Proposed changes over time
  - Top improvement categories
  - Impact assessment distribution
  - Metrics trending

- [ ] **System Health Dashboard**
  - CPU/memory metrics timeline
  - Mode transitions
  - Consolidation triggers
  - Resource threshold violations

**Export Configuration:**
- [ ] Export dashboard JSON to `config/kibana/dashboards/`
- [ ] Document import instructions

---

### Part 2: Data Lifecycle Management (Days 3-4)

#### 2.1 Retention Policy Configuration

**New File:** `src/personal_agent/telemetry/lifecycle.py`

```python
from datetime import timedelta
from pydantic import BaseModel

class RetentionPolicy(BaseModel):
    """Retention policy for a data type."""
    name: str
    hot_duration: timedelta  # Keep locally, indexed
    warm_duration: timedelta  # Archive, compressed
    cold_duration: timedelta  # Delete after this
    archive_enabled: bool = True

# Default policies (dev environment - 2 week archival)
RETENTION_POLICIES = {
    "file_logs": RetentionPolicy(
        name="File Logs",
        hot_duration=timedelta(days=7),
        warm_duration=timedelta(days=14),  # 2 week archival
        cold_duration=timedelta(days=30),
    ),
    "captains_log_captures": RetentionPolicy(
        name="Task Captures",
        hot_duration=timedelta(days=14),  # 2 weeks hot
        warm_duration=timedelta(days=14),  # 2 week archive
        cold_duration=timedelta(days=90),  # Delete after 90d
    ),
    "captains_log_reflections": RetentionPolicy(
        name="Reflections",
        hot_duration=timedelta(days=14),  # 2 weeks hot
        warm_duration=timedelta(days=14),  # 2 week archive
        cold_duration=timedelta(days=180),  # Keep 6 months
    ),
    "elasticsearch_logs": RetentionPolicy(
        name="ES Event Logs",
        hot_duration=timedelta(days=14),
        warm_duration=timedelta(days=14),
        cold_duration=timedelta(days=30),  # ES auto-deletes
        archive_enabled=False,  # Already in ES
    ),
    "neo4j_graph": RetentionPolicy(
        name="Knowledge Graph",
        hot_duration=timedelta(days=365),
        warm_duration=timedelta(days=730),
        cold_duration=timedelta(days=0),  # Never delete
        archive_enabled=False,  # Graph is the archive
    ),
}
```

**Configuration:**
- [ ] Add retention policy settings to `.env`
- [ ] Document policy rationale in `docs/DATA_LIFECYCLE.md`

#### 2.2 Data Lifecycle Manager

**New File:** `src/personal_agent/telemetry/lifecycle_manager.py`

```python
class DataLifecycleManager:
    """Manages data retention, archival, and cleanup."""

    async def check_disk_usage(self) -> DiskUsageReport:
        """Check disk usage for all data stores."""

    async def archive_old_data(self, data_type: str) -> ArchiveResult:
        """Archive data past hot_duration."""

    async def purge_expired_data(self, data_type: str) -> PurgeResult:
        """Delete data past cold_duration."""

    async def cleanup_elasticsearch_indices(self) -> ESCleanupResult:
        """Apply ILM policy to ES indices."""

    async def generate_report(self) -> LifecycleReport:
        """Generate data lifecycle status report."""
```

**Tasks:**
- [ ] Implement disk usage monitoring
- [ ] Implement file-based archival (gzip compression)
- [ ] Implement deletion with safety checks
- [ ] Implement ES Index Lifecycle Management (ILM)
- [ ] Add telemetry events for all lifecycle operations

#### 2.3 Brainstem Scheduler Integration

**File to Modify:** `src/personal_agent/brainstem/scheduler.py`

- [ ] Add lifecycle check to scheduler (hourly)
- [ ] Add archive task to scheduler (daily at 2AM)
- [ ] Add purge task to scheduler (weekly Sunday 3AM)
- [ ] Add disk usage alerts (>80% threshold)

**Testing:**
- [ ] Test archive creates compressed files
- [ ] Test purge respects retention policies
- [ ] Test disk usage monitoring
- [ ] Test ES ILM policy application

---

### Part 3: Adaptive Threshold Tuning (Days 5-6)

#### 3.1 Threshold Optimizer

**New File:** `src/personal_agent/brainstem/optimizer.py`

```python
class ThresholdOptimizer:
    """Analyzes usage patterns and proposes threshold adjustments."""

    async def analyze_resource_patterns(
        self, days: int = 7
    ) -> ResourceAnalysis:
        """Query ES for CPU/memory patterns."""

    async def detect_false_positives(self) -> FalsePositiveReport:
        """Find unnecessary mode transitions."""

    async def propose_threshold_adjustment(
        self, metric: str
    ) -> ThresholdProposal:
        """Generate data-backed threshold proposal."""

    async def run_ab_test(
        self, proposal: ThresholdProposal
    ) -> ABTestResult:
        """Test new threshold in shadow mode."""
```

**Metrics to Optimize:**
- [ ] Second Brain CPU threshold (currently 50%)
- [ ] Second Brain memory threshold (currently 70%)
- [ ] Second Brain idle time (currently 300s)
- [ ] Consolidation min interval (currently 3600s)

**Captain's Log Integration:**
- [ ] Generate Captain's Log proposal for threshold changes
- [ ] Include supporting metrics from ES queries
- [ ] Add A/B test results to proposal

#### 3.2 Pattern Analysis Queries

**New File:** `src/personal_agent/telemetry/queries.py`

```python
class TelemetryQueries:
    """Common analytics queries for ES data."""

    async def get_resource_percentiles(
        self, metric: str, days: int
    ) -> dict[str, float]:
        """Get p50, p75, p90, p95, p99 for metric."""

    async def get_mode_transitions(
        self, days: int
    ) -> list[ModeTransition]:
        """Get all mode transitions with context."""

    async def get_consolidation_triggers(
        self, days: int
    ) -> list[ConsolidationEvent]:
        """Get all consolidation events."""

    async def get_task_patterns(
        self, days: int
    ) -> TaskPatternReport:
        """Analyze task execution patterns."""
```

**Testing:**
- [ ] Test resource percentile calculations
- [ ] Test mode transition queries
- [ ] Test pattern detection accuracy
- [ ] Test proposal generation

---

### Part 4: Feedback Loop for Quality (Days 7-8)

#### 4.1 Consolidation Quality Monitor

**New File:** `src/personal_agent/second_brain/quality_monitor.py`

```python
class ConsolidationQualityMonitor:
    """Monitors entity extraction and graph quality."""

    async def check_entity_extraction_quality(self) -> QualityReport:
        """Analyze entity extraction patterns."""
        # Metrics:
        # - Entities per conversation ratio
        # - Entity name length distribution
        # - Duplicate entity detection

    async def check_graph_health(self) -> GraphHealthReport:
        """Analyze Neo4j graph structure."""
        # Metrics:
        # - Relationship density
        # - Orphaned nodes
        # - Entity clustering
        # - Temporal gaps

    async def detect_anomalies(self) -> list[Anomaly]:
        """Detect unusual patterns."""
        # Examples:
        # - Sudden spike in entity count
        # - No relationships created
        # - Entity extraction failures
```

**Quality Metrics:**
- [ ] Entity-to-conversation ratio (target: 0.5-2.0)
- [ ] Relationship density (target: 1.0-3.0)
- [ ] Duplicate entity rate (target: <5%)
- [ ] Extraction failure rate (target: <1%)

#### 4.2 User Query Success Tracking

**File to Modify:** `src/personal_agent/memory/service.py`

- [ ] Track query result counts
- [ ] Track relevance scores
- [ ] Track user feedback (implicit: did they rephrase?)
- [ ] Send metrics to ES for analysis

**Testing:**
- [ ] Test quality metric calculations
- [ ] Test anomaly detection
- [ ] Test feedback signal collection

---

### Part 5: Proactive Insights Engine (Days 9-10)

#### 5.1 Cross-Data Pattern Analysis

**New File:** `src/personal_agent/insights/engine.py`

```python
class InsightsEngine:
    """Generates insights from cross-data analysis."""

    async def analyze_patterns(self) -> list[Insight]:
        """Find patterns across all data sources."""
        # Data sources:
        # - ES logs (telemetry)
        # - ES captures (tasks)
        # - ES reflections (proposals)
        # - Neo4j (knowledge graph)
        # - PostgreSQL (cost tracking)

    async def detect_cost_anomalies(self) -> list[CostAnomaly]:
        """Find unusual spending patterns."""

    async def suggest_improvements(self) -> list[Improvement]:
        """Generate improvement suggestions."""

    async def create_captain_log_proposals(
        self, insights: list[Insight]
    ) -> list[CaptainLogEntry]:
        """Convert insights to proposals."""
```

**Insight Types:**
- [ ] **Resource Insights**: "CPU high when tool X runs"
- [ ] **Cost Insights**: "Consolidation costs $0.50/day"
- [ ] **Quality Insights**: "Entity duplication increasing"
- [ ] **Usage Insights**: "Most tasks at 9AM-11AM"

#### 5.2 Scheduled Insight Generation

**Integration with Scheduler:**
- [ ] Daily: Generate insights report (6AM)
- [ ] Weekly: Create Captain's Log proposals (Sunday 9AM)
- [ ] Monthly: Generate cost analysis report

**Testing:**
- [ ] Test pattern detection with synthetic data
- [ ] Test insight generation quality
- [ ] Test Captain's Log proposal creation

---

## ğŸ§ª Testing Strategy

### Unit Tests

**New Test Files:**
- [ ] `tests/test_telemetry/test_lifecycle_manager.py`
- [ ] `tests/test_telemetry/test_queries.py`
- [ ] `tests/test_brainstem/test_optimizer.py`
- [ ] `tests/test_second_brain/test_quality_monitor.py`
- [ ] `tests/test_insights/test_engine.py`

### Integration Tests

- [ ] Test Captain's Log â†’ ES streaming
- [ ] Test data archival and restoration
- [ ] Test threshold optimization end-to-end
- [ ] Test quality monitoring with real graph
- [ ] Test insights generation with real data

### Manual Tests

- [ ] Run system for 7 days, verify lifecycle operations
- [ ] Test Kibana dashboards with real data
- [ ] Verify disk usage stays under limits
- [ ] Test proposal generation quality
- [ ] Validate A/B testing framework

---

## ğŸ“Š Success Metrics

### Phase 2.3 Completion Criteria

- [ ] **Telemetry**: All Captain's Log data in Elasticsearch
- [ ] **Dashboards**: 3 Kibana dashboards operational
- [ ] **Retention**: Policies configured and enforced
- [ ] **Cleanup**: Automated archival/purge working
- [ ] **Monitoring**: Disk usage alerts configured
- [ ] **Optimization**: Threshold proposals generated
- [ ] **Quality**: Graph health metrics tracked
- [ ] **Insights**: Weekly proposals automated

### Key Performance Indicators

| Metric | Target | Measurement |
|--------|--------|-------------|
| Disk usage growth | <1GB/month | Monitor telemetry directory size |
| ES index size | <10GB total | Query ES cluster stats |
| Cleanup success rate | >99% | Track lifecycle operation failures |
| Threshold proposal accuracy | >70% approval | Track Captain's Log reviews |
| Graph quality score | >0.8 | Entity/relationship density metrics |
| Insight relevance | >50% actionable | Manual review weekly |
| Data loss incidents | 0 | Archive restoration tests |

---

## ğŸ—‚ï¸ File Structure

```
src/personal_agent/
â”œâ”€â”€ telemetry/
â”‚   â”œâ”€â”€ lifecycle.py              # NEW: Retention policies
â”‚   â”œâ”€â”€ lifecycle_manager.py      # NEW: Data lifecycle orchestration
â”‚   â””â”€â”€ queries.py                # NEW: Analytics query library
â”œâ”€â”€ brainstem/
â”‚   â””â”€â”€ optimizer.py              # NEW: Threshold optimization
â”œâ”€â”€ second_brain/
â”‚   â””â”€â”€ quality_monitor.py        # NEW: Consolidation quality
â”œâ”€â”€ insights/
â”‚   â”œâ”€â”€ __init__.py               # NEW: Insights package
â”‚   â”œâ”€â”€ engine.py                 # NEW: Pattern analysis
â”‚   â””â”€â”€ models.py                 # NEW: Insight data models
â””â”€â”€ captains_log/
    â”œâ”€â”€ capture.py                # MODIFY: Add ES streaming
    â””â”€â”€ manager.py                # MODIFY: Add ES streaming

config/
â”œâ”€â”€ retention_policies.yaml       # NEW: Retention config
â””â”€â”€ kibana/
    â””â”€â”€ dashboards/               # NEW: Dashboard exports
        â”œâ”€â”€ task_analytics.json
        â”œâ”€â”€ reflection_insights.json
        â””â”€â”€ system_health.json

docs/
â”œâ”€â”€ DATA_LIFECYCLE.md             # NEW: Retention documentation
â”œâ”€â”€ INSIGHTS_ENGINE.md            # NEW: Insights architecture
â””â”€â”€ KIBANA_DASHBOARDS.md          # NEW: Dashboard guide

tests/
â”œâ”€â”€ test_telemetry/
â”‚   â”œâ”€â”€ test_lifecycle_manager.py # NEW
â”‚   â””â”€â”€ test_queries.py           # NEW
â”œâ”€â”€ test_brainstem/
â”‚   â””â”€â”€ test_optimizer.py         # NEW
â”œâ”€â”€ test_second_brain/
â”‚   â””â”€â”€ test_quality_monitor.py   # NEW
â””â”€â”€ test_insights/
    â””â”€â”€ test_engine.py            # NEW
```

---

## ğŸ“… Implementation Timeline

### Week 1: Telemetry & Lifecycle (Days 1-4)

**Days 1-2: Elasticsearch Integration**
- Implement Captain's Log â†’ ES streaming
- Create Kibana dashboards
- Test data flow

**Days 3-4: Data Lifecycle**
- Implement retention policies
- Build lifecycle manager
- Test archival and cleanup

**Deliverable:** All telemetry in ES, automated cleanup operational

---

### Week 2: Optimization & Insights (Days 5-10)

**Days 5-6: Adaptive Thresholds**
- Build threshold optimizer
- Implement pattern analysis queries
- Test proposal generation

**Days 7-8: Quality Monitoring**
- Build consolidation quality monitor
- Track user query success
- Detect anomalies

**Days 9-10: Insights Engine**
- Implement cross-data analysis
- Build insight generation
- Integrate with Captain's Log

**Deliverable:** Self-optimizing system with proactive insights

---

## ğŸ”§ Configuration

### New Environment Variables

```bash
# Data Lifecycle (dev environment - 2 week archival)
AGENT_RETENTION_HOT_DAYS=14         # Keep local & indexed
AGENT_RETENTION_WARM_DAYS=14        # Archive compressed (2 weeks)
AGENT_RETENTION_COLD_DAYS=90        # Delete after 90 days
AGENT_ARCHIVE_ENABLED=true          # Enable archival
AGENT_DISK_USAGE_ALERT_PERCENT=80   # Alert threshold

# Threshold Optimization
AGENT_OPTIMIZER_ENABLED=true        # Enable auto-optimization
AGENT_OPTIMIZER_AB_TEST_DAYS=7      # A/B test duration
AGENT_OPTIMIZER_MIN_CONFIDENCE=0.7  # Minimum confidence

# Quality Monitoring
AGENT_QUALITY_CHECK_INTERVAL=3600   # Check every hour
AGENT_QUALITY_ALERT_THRESHOLD=0.6   # Alert if score <0.6

# Insights Engine
AGENT_INSIGHTS_ENABLED=true         # Enable insights
AGENT_INSIGHTS_DAILY_RUN_HOUR=6     # Generate at 6AM
AGENT_INSIGHTS_WEEKLY_DAY=0         # Sunday
```

---

## ğŸš¨ Risk Management

### Potential Issues

1. **Data Loss During Cleanup**
   - **Mitigation**: Test archival/restore before purge
   - **Rollback**: Keep 7-day safety window before deletion

2. **Elasticsearch Storage Growth**
   - **Mitigation**: ILM policies with auto-delete
   - **Monitoring**: Daily cluster size checks

3. **False Positive Threshold Changes**
   - **Mitigation**: A/B testing with shadow mode
   - **Rollback**: Revert flag + Captain's Log tracking

4. **Insight Spam (Too Many Proposals)**
   - **Mitigation**: Confidence threshold + rate limiting
   - **Tuning**: Adjust insight generation frequency

5. **Performance Impact (Analytics Queries)**
   - **Mitigation**: Run during idle time only
   - **Optimization**: Cache results, incremental updates

---

## ğŸ“– Documentation Updates

### Required Documentation

- [ ] `docs/DATA_LIFECYCLE.md`
  - Retention policy rationale
  - Archival process
  - Restoration procedures

- [ ] `docs/INSIGHTS_ENGINE.md`
  - Architecture overview
  - Pattern detection algorithms
  - Insight types and confidence scores

- [ ] `docs/KIBANA_DASHBOARDS.md`
  - Dashboard descriptions
  - Import/export procedures
  - Custom query examples

- [ ] `docs/THRESHOLD_OPTIMIZATION.md`
  - How optimization works
  - A/B testing methodology
  - How to review proposals

### Updated AGENTS.md Files

- [ ] `src/personal_agent/telemetry/AGENTS.md`
  - Add lifecycle management section
  - Document analytics queries

- [ ] `src/personal_agent/brainstem/AGENTS.md`
  - Add optimizer documentation
  - Update scheduler integration

- [ ] `src/personal_agent/insights/AGENTS.md`
  - New file for insights package

---

## ğŸ¯ Phase 2.3 Completion Criteria

### Must Have âœ…

- [x] Captain's Log data in Elasticsearch
- [x] Retention policies configured and enforced
- [x] Automated cleanup operational
- [x] Basic threshold optimization working
- [x] Quality monitoring implemented
- [x] At least 1 Kibana dashboard

### Should Have ğŸ¯

- [ ] All 3 Kibana dashboards
- [ ] A/B testing framework
- [ ] Insights engine generating proposals
- [ ] Disk usage monitoring and alerts
- [ ] ES ILM policies active

### Nice to Have ğŸŒŸ

- [ ] Advanced pattern detection
- [ ] Automated threshold rollback
- [ ] Cost prediction models
- [ ] User feedback integration
- [ ] Anomaly alerting

---

## ğŸ”„ Next Phase: Phase 2.4

**Potential Focus Areas:**
1. **Advanced Learning**: Model fine-tuning based on user corrections
2. **Multi-Modal Memory**: Images, audio, video in knowledge graph
3. **Collaborative Features**: Share knowledge graph with other instances
4. **Advanced Governance**: Context-aware permissions, risk scoring
5. **UI Enhancement**: Web dashboard, mobile companion

---

## ğŸ“ Notes

- Keep Phase 2.3 scope manageable (10 days)
- Prioritize data safety over features
- Test lifecycle operations extensively
- Document everything for maintenance
- Get user feedback on insights quality

**Remember:** Homeostasis is about sustainability. The system should regulate itself with minimal manual intervention.

---

**Phase Owner**: AI Agent
**Reviewer**: Project Owner (Alex)
**Status**: Ready for review and approval

---

## âœ… Pre-Implementation Checklist

- [ ] Review plan with project owner
- [ ] Validate retention policy durations
- [ ] Confirm Elasticsearch storage capacity
- [ ] Test archival on sample data
- [ ] Document rollback procedures
- [ ] Create backup of current data
- [ ] Set up monitoring alerts

**DO NOT START IMPLEMENTATION UNTIL APPROVED**
