# Safety Architectures for LLM-Based Agents and Sandboxes

## Introduction

Large Language Model (LLM)-based agents are AI systems that use an LLM as a core reasoning component and can **perceive and act** on their environment through tools or APIs. This new agentic capability unlocks powerful multi-step task execution, but it also **vastly expands the attack surface and safety concerns** [oai_citation:0‡sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S1566253525010036#:~:text=However%2C%20the%20increasing%20complexity%20and,internally%2C%20compromising%20the%20agent%E2%80%99s%20function). Unlike a static text-only LLM, an agent operates over multiple layers – prompt interpretation, multi-turn planning, tool invocation, memory, and action execution – each introducing potential failure modes and vulnerabilities [oai_citation:1‡sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S1566253525010036#:~:text=However%2C%20the%20increasing%20complexity%20and,broadens%20further%20and%20undergoes%20a). Researchers and industry practitioners from 2023–2025 have recognized numerous threats such as *prompt injections*, *jailbreaks*, *unsafe or malicious tool use*, *memory poisoning*, *data leakage*, and *erroneous or harmful action planning* [oai_citation:2‡sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S1566253525010036#:~:text=However%2C%20the%20increasing%20complexity%20and,internally%2C%20compromising%20the%20agent%E2%80%99s%20function) [oai_citation:3‡github.com](https://github.com/Ymm-cll/TrustAgent#:~:text=,leakage%2C%20hallucinations%2C%20and%20fairness%20biases). These risks can lead an agent to cause real harm (e.g. leaking sensitive data, damaging files, financial losses) or to be manipulated by adversaries. To mitigate these issues, a range of **safety architectures** have been proposed – from academic frameworks that introduce oversight and structured policies, to industry “AI sandbox” environments that isolate and monitor agent behavior. This survey reviews the recent literature on LLM agent security, focusing on how different approaches define threat models, intercept or emulate tool calls, design supervisors or policy engines, and log/audit agent behavior. We then distill a set of concrete design patterns for building a safer LLM-based assistant, including a deterministic supervisor (kill-switch), a robust outbound policy filter, and a local operational sandbox for running an agent with least privilege.

## Security Surveys and Threat Models for LLM Agents (2023–2025)

Several comprehensive **surveys in 2023–2025** characterize the security and privacy issues of LLM-based agents and propose taxonomies of threats and defenses. For example, Tang *et al.* (2025) outline how an LLM-agent’s **multi-layered architecture** inherently broadens its threat landscape [oai_citation:4‡sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S1566253525010036#:~:text=However%2C%20the%20increasing%20complexity%20and,broadens%20further%20and%20undergoes%20a). Because an agent maintains *state* and can take *actions* (e.g. executing code or web requests), subtle perturbations can cascade into serious failures [oai_citation:5‡sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S1566253525010036#:~:text=output%20processing%2C%20the%20LLM,internally%2C%20compromising%20the%20agent%E2%80%99s%20function). Their survey enumerates threats including prompt/constitution **injections**, **jailbreaks** (bypassing safety instructions), **unsafe tool usage** (issuing harmful API or shell commands), **memory poisoning** (introducing false or malicious context into the agent’s knowledge), and reasoning errors leading to **unsafe plans** [oai_citation:6‡sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S1566253525010036#:~:text=However%2C%20the%20increasing%20complexity%20and,internally%2C%20compromising%20the%20agent%E2%80%99s%20function). In multi-agent settings, they note additional risks in agent-to-agent communication channels and shared memory that could allow one compromised agent to influence others [oai_citation:7‡sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S1566253525010036#:~:text=horizon%20planning.%20This%20multi,hand%2C%20their%20autonomy%20and%20tool).

Another notable effort is *TrustAgent: A Survey on Trustworthy LLM Agents* (2024) [oai_citation:8‡github.com](https://github.com/Ymm-cll/TrustAgent#:~:text=Survey). This work defines an **intrinsic vs. extrinsic** trustworthiness taxonomy: “intrinsic” issues pertain to the agent’s own components (the *LLM brain*, *memory*, *tool interface*), while “extrinsic” issues cover the agent’s interactions with others (users, other agents, environment) [oai_citation:9‡github.com](https://github.com/Ymm-cll/TrustAgent#:~:text=1%EF%B8%8F%E2%83%A3%20Intrinsic%20Trustworthiness). For instance, intrinsic threats include *prompt hijacking* of the LLM’s instructions, *tool misuse or abuse* (e.g. calling unauthorized APIs or using tools in unintended ways), and *memory leaks* of private data [oai_citation:10‡github.com](https://github.com/Ymm-cll/TrustAgent#:~:text=,Tool%20Abuse%2C%20Malicious%20API%20Calls). Extrinsic threats include *indirect prompt injections* (where attackers manipulate the environment or inputs to trick the agent) and *cooperative attacks* in multi-agent systems [oai_citation:11‡github.com](https://github.com/Ymm-cll/TrustAgent#:~:text=2%EF%B8%8F%E2%83%A3%20Extrinsic%20Trustworthiness). The TrustAgent survey’s taxonomy highlights not only overt security failures (like executing malicious actions) but also broader trust issues such as hallucinations or fairness bias [oai_citation:12‡github.com](https://github.com/Ymm-cll/TrustAgent#:~:text=,leakage%2C%20hallucinations%2C%20and%20fairness%20biases) – underscoring that an unsafe agent may not just cause security incidents but also violate ethical or fairness norms.

**Threat models** in these surveys generally assume that an agent may face *malicious user input*, *hostile environmental content* (e.g. compromised websites or files that the agent processes), or may itself take *undesirable initiatives* under broad goals. A recurring theme is *indirect prompt injection*, where instructions hidden in the environment (web pages, files, emails) can cause the agent to deviate from its policy [oai_citation:13‡aclanthology.org](https://aclanthology.org/anthology-files/anthology-files/pdf/findings/2024.findings-emnlp.79.pdf#:~:text=deemed%20unsafe%2C%20thereby%20preventing%20safety,and%20AgentMonitor%20utilized%20LLMs%20as) [oai_citation:14‡positive.security](https://positive.security/blog/auto-gpt-rce#:~:text=Finding%20places%20where%20the%20LLM,it%20is%20looking%20for%20more). Attackers might exploit an agent’s tool access by feeding crafted content that causes the agent to run harmful code or reveal secrets [oai_citation:15‡positive.security](https://positive.security/blog/auto-gpt-rce#:~:text=Finding%20places%20where%20the%20LLM,content%20inside%20of%20an%20iframe) [oai_citation:16‡positive.security](https://positive.security/blog/auto-gpt-rce#:~:text=Convincing%20GPT,This). The worst-case assumption is that, without safeguards, an LLM agent given powerful tools could **execute arbitrary commands**, exfiltrate data, or damage its operating environment. Therefore, any safety architecture must define controls to limit the agent’s actions and exposure *before* such harm occurs.

## Frameworks for Safe and Trustworthy LLM Agents

**Research frameworks** propose modular architectures to make LLM-based agents safer. These typically introduce a form of *supervisory mechanism* or *policy enforcement* alongside the base LLM. We discuss a few representative approaches below, focusing on how each handles tool use, oversight, and auditing.

### Agent Constitution and Safe Planning: *TrustAgent*

One influential approach is **TrustAgent** (Yu *et al.*, 2024), which embeds a **Constitution** of safety rules into the agent’s cognition [oai_citation:17‡emergentmind.com](https://www.emergentmind.com/papers/2402.01586#:~:text=integrated%20into%20high,agent%27s%20safety%20across%20multiple%20domains) [oai_citation:18‡emergentmind.com](https://www.emergentmind.com/papers/2402.01586#:~:text=At%20the%20core%20of%20TrustAgent,before%20execution%20through%20comprehensive%20inspections). The threat model here is an agent tasked with open-ended goals (e.g. “plan and execute steps to accomplish X”) that might come up with an unsafe plan or action if left unconstrained. TrustAgent’s solution is an *Agent Constitution* – a fixed set of principles or prohibitions (derived from legal, ethical, and domain guidelines) that the agent **must not violate** [oai_citation:19‡emergentmind.com](https://www.emergentmind.com/papers/2402.01586#:~:text=At%20the%20core%20of%20TrustAgent,before%20execution%20through%20comprehensive%20inspections). These constitutional rules are enforced through a *three-stage planning process* [oai_citation:20‡emergentmind.com](https://www.emergentmind.com/papers/2402.01586#:~:text=TrustAgent%2C%20with%20a%20particular%20focus,improves%20safety%20but%20also%20enhances):

- **Pre-planning injection:** Before the agent even begins formulating a plan, it is primed with safety knowledge and constitutional rules. This might involve appending a detailed list of “Dos and Don’ts” (the policy) to the prompt so that the LLM plans with those constraints in mind [oai_citation:21‡emergentmind.com](https://www.emergentmind.com/papers/2402.01586#:~:text=TrustAgent%2C%20with%20a%20particular%20focus,agent%27s%20safety%20across%20multiple%20domains). For example, a rule might state “Never call an API that charges money without user approval” or “Do not delete or corrupt user files.” By seeding such knowledge up front, the agent’s *initial* plan is more likely to avoid known hazardous actions.

- **In-planning oversight:** During the plan generation, the framework monitors or guides the LLM’s chain-of-thought to steer it away from unsafe options [oai_citation:22‡emergentmind.com](https://www.emergentmind.com/papers/2402.01586#:~:text=TrustAgent%2C%20with%20a%20particular%20focus,improves%20safety%20but%20also%20enhances). The TrustAgent paper suggests injecting safety checks at each reasoning step (e.g. asking the model to explain *why* each step is safe) or using an intermediate “critic” model that flags potentially dangerous actions before they are solidified in the plan. This *step-by-step interception* helps catch unsafe action chains (sequences of steps that might individually seem fine but collectively lead to a bad outcome [oai_citation:23‡github.com](https://github.com/Ymm-cll/TrustAgent#:~:text=,leakage%2C%20hallucinations%2C%20and%20fairness%20biases)).

- **Post-planning inspection:** Once a full plan or tool call sequence is proposed by the agent, a supervisor module performs a final **safety audit** before execution [oai_citation:24‡emergentmind.com](https://www.emergentmind.com/papers/2402.01586#:~:text=TrustAgent%2C%20with%20a%20particular%20focus,improves%20safety%20but%20also%20enhances). This could be an automated scan for disallowed actions (matching against the Constitution rules) or even an LLM-based checker that evaluates “Does this plan violate any rule or pose any risk?” Any flagged issues can cause the plan to be rejected or revised. Only a plan that passes this inspection is allowed to execute.

By inserting these checks, TrustAgent **intercepts tool calls** at the planning stage *before* they happen in the real world. If the agent’s plan included a call to an unsafe tool or parameter (say, attempting to run a shell command to wipe a directory), the post-planning inspector would catch it and prevent execution. In effect, the real tool calls are gated behind a policy check. This design proved effective in experiments – TrustAgent significantly reduced dangerous behaviors in agents like GPT-4, without sacrificing task performance [oai_citation:25‡emergentmind.com](https://www.emergentmind.com/papers/2402.01586#:~:text=,for%20future%20trustworthy%20AI%20designs) [oai_citation:26‡emergentmind.com](https://www.emergentmind.com/papers/2402.01586#:~:text=Using%20several%20closed%20and%20open,safer%20and%20more%20useful%20actions). The authors report that agents guided by an Agent Constitution maintained high *helpfulness* while adhering to safety rules, implying that strong safety need not come at the cost of capability [oai_citation:27‡emergentmind.com](https://www.emergentmind.com/papers/2402.01586#:~:text=,for%20future%20trustworthy%20AI%20designs). One key insight was the importance of the base LLM’s reasoning quality: a smarter LLM (GPT-4) adhered to the Constitution far better than smaller models [oai_citation:28‡emergentmind.com](https://www.emergentmind.com/papers/2402.01586#:~:text=highest%20safety%20awareness%20and%20helpfulness,safer%20and%20more%20useful%20actions), highlighting that oversight mechanisms work best in tandem with robust reasoning faculties.

In terms of **logging and audit**, a Constitution-based agent naturally produces a trace of its safety checks. TrustAgent’s framework can log each stage: what safety prompts were injected, where an in-flight plan was altered due to a rule, and the results of the final plan inspection. These logs are invaluable for auditing *why* an agent was stopped or how a potential failure was mitigated. They also help refine the Constitution itself by revealing new failure modes: if an agent came up with a dangerous action that wasn’t covered by existing rules, designers can add a new rule and see it reflected in the logged oversight process.

### Tool Execution Emulation and Safe Testing: *ToolEmu*

While frameworks like TrustAgent focus on guiding a live agent’s behavior, *ToolEmu* (Ruan *et al.*, 2024) addresses the challenge of **evaluating agent safety in diverse scenarios** [oai_citation:29‡arxiv.org](https://arxiv.org/abs/2309.15817#:~:text=,range%20of%20tools%20and%20scenarios). The premise is that manually testing an autonomous agent against every possible risky situation is labor-intensive and potentially perilous (you wouldn’t want to actually execute a harmful action just to see what happens). **ToolEmu** defines a threat model of *long-tail risks*: scenarios where an agent with tool access might cause severe harm – e.g., revealing a private document via an API, or making an unwise financial transaction [oai_citation:30‡arxiv.org](https://arxiv.org/abs/2309.15817#:~:text=,to%20emulate%20tool%20execution%20and). Instead of hooking the agent up to real tools and hoping nothing catastrophic occurs during tests, ToolEmu **intercepts tool calls by emulating them with a language model** [oai_citation:31‡arxiv.org](https://arxiv.org/abs/2309.15817#:~:text=difficult%20to%20find%20high,we%20provide%20a%20quantitative%20risk).

In practice, ToolEmu provides a suite of *virtual tools* (like a fake file system, a fake web browser, a mock bank API, etc.), all implemented by an LLM that *pretends* to be those tools [oai_citation:32‡arxiv.org](https://arxiv.org/abs/2309.15817#:~:text=difficult%20to%20find%20high,we%20provide%20a%20quantitative%20risk). When the agent under test issues a tool command (say `transfer_money(amount=1000)`), the call is not sent to an actual banking system. Instead, an LLM (e.g. GPT-4) that knows the scenario will generate a plausible outcome – maybe an error, or a success message, or even an unexpected event (like “Account balance too low”). This way, one can simulate dangerous situations safely: e.g., testing if the agent would try to transfer more money than allowed, or how it handles being told the transfer succeeded. By varying the emulated tool responses, testers can see if the agent has **unsafe failure modes** – for instance, does it retry a risky action repeatedly, or escalate to even more dangerous steps?

Crucially, ToolEmu pairs the emulator with an **automatic safety evaluator**, which is another LLM that examines the agent’s behaviors in these simulated scenarios [oai_citation:33‡arxiv.org](https://arxiv.org/abs/2309.15817#:~:text=enables%20the%20testing%20of%20LM,9). This evaluator looks at the agent’s action traces and outcomes to judge if a failure occurred and how severe it might be [oai_citation:34‡arxiv.org](https://arxiv.org/abs/2309.15817#:~:text=enables%20the%20testing%20of%20LM,9). For example, if the agent, even in a sandbox, attempted to leak a password or make an irreversible action, the evaluator flags that as a serious safety violation. In their results, Ruan *et al.* report that a majority (68.8%) of the failures identified via the LM-emulated tools reflected genuine real-world risks [oai_citation:35‡arxiv.org](https://arxiv.org/abs/2309.15817#:~:text=automatic%20safety%20evaluator%20that%20examines,the%20need%20to%20develop%20safer). Even the best agent they tested (one tuned for caution) still exhibited unsafe behaviors nearly 24% of the time under certain conditions [oai_citation:36‡arxiv.org](https://arxiv.org/abs/2309.15817#:~:text=evaluation%20and%20find%20that%2068.8,world%20deployment). This underscores that current agents, if given tools, **will make dangerous mistakes** unless we systematically root them out.

In summary, ToolEmu doesn’t directly supervise a deployed agent – rather, it provides a **sandboxed testing framework**. Its pattern for intercepting tool calls (LM-based emulation) is powerful for breadth of testing, since you can quickly spin up new “tools” or situations by describing them to the emulator LLM. The **threat model** is defined by the library of tools and scenarios: for each potential high-stakes tool (file system, shell, network, etc.), the testers define what “unsafe” means (deleting files, exfiltrating data, etc.) and create scenarios to probe that. **Logging and auditing** in ToolEmu are central: every interaction – the agent’s thought process, the emulated tool I/O, and the safety evaluator’s judgment – is recorded [oai_citation:37‡aclanthology.org](https://aclanthology.org/anthology-files/anthology-files/pdf/findings/2024.findings-emnlp.79.pdf#:~:text=LLM%20agents%E2%80%99%20action%20execution,2024) [oai_citation:38‡aclanthology.org](https://aclanthology.org/anthology-files/anthology-files/pdf/findings/2024.findings-emnlp.79.pdf#:~:text=Though%20implemented%20with%20different%20settings%2C,of%20environments%2C%20it%20remains%20unclear). These logs allow researchers to identify exactly which prompt or decision led to a failure. Ultimately, frameworks like ToolEmu inform the design of better agents by revealing flaws *before* deployment and by providing data to develop fixes (e.g., fine-tuning the agent to avoid certain actions or improving the prompt instructions).

### Run-time Action Monitoring and Intervention: *AgentMonitor*

Another line of research focuses on a **real-time “safety monitor”** that runs alongside an agent during operation. *AgentMonitor* (Naihin *et al.*, 2023) exemplifies this approach [oai_citation:39‡arxiv.org](https://arxiv.org/abs/2311.10538#:~:text=wild.%20Yet%20real,an%20adversarial%20simulated%20agent%2C%20we). Here, the context is testing autonomous agents “in the wild” (e.g. letting an AutoGPT system roam the real internet or interact with live systems) – something that is inherently risky. AgentMonitor assumes the threat model that *even during a test run*, an agent might stumble into harmful behavior or external adversaries (e.g. a malicious website) might try to exploit the agent [oai_citation:40‡arxiv.org](https://arxiv.org/abs/2311.10538#:~:text=,that%20is%20flexible%20enough%20to). To mitigate this, AgentMonitor introduces a **context-sensitive supervisor** that continuously observes the agent’s “thoughts” (the chain-of-thought and intended actions) and can **terminate the agent’s run if an unsafe action is imminent** [oai_citation:41‡arxiv.org](https://arxiv.org/abs/2311.10538#:~:text=wild.%20Yet%20real,an%20adversarial%20simulated%20agent%2C%20we). Essentially, it’s a watchdog process with the authority to hit the emergency stop.

Concretely, the AgentMonitor is implemented as an LLM-based classifier plus rule-based checks that evaluate each proposed action in context [oai_citation:42‡aclanthology.org](https://aclanthology.org/anthology-files/anthology-files/pdf/findings/2024.findings-emnlp.79.pdf#:~:text=LLM%20agents%E2%80%99%20action%20execution,2024) [oai_citation:43‡aclanthology.org](https://aclanthology.org/anthology-files/anthology-files/pdf/findings/2024.findings-emnlp.79.pdf#:~:text=Though%20implemented%20with%20different%20settings%2C,of%20environments%2C%20it%20remains%20unclear). For example, if the agent decides “I should run command: `sudo rm -rf /` to free space,” the monitor would catch that (it recognizes the action is a dangerous shell command) and immediately halt execution. The monitor might also watch for actions that violate a policy (like accessing disallowed URLs, or outputting private data). Naihin *et al.* describe the monitor as **flexible and context-sensitive** – meaning it considers not just the action in isolation, but the situation. An action that’s usually fine (like “open browser to `<url>`”) could be unsafe if the URL is known malicious or if the agent is in a sensitive context. The monitor uses the agent’s accumulated context to gauge this, and it can stop the agent “at any point to prevent it from taking unsafe actions” [oai_citation:44‡arxiv.org](https://arxiv.org/abs/2311.10538#:~:text=of%20encountering%20new%20unsafe%20agent,of%20AutoGPT%2C%20and%20we%20identify).

A noteworthy aspect of AgentMonitor is the emphasis on **logging and human audit**. Rather than simply blocking an action and letting the agent continue, AgentMonitor will typically abort the entire agent test and produce a report of what it found concerning [oai_citation:45‡arxiv.org](https://arxiv.org/abs/2311.10538#:~:text=wild.%20Yet%20real,that%20is%20flexible%20enough%20to). It *ranks suspect behavior and logs it for human examination* [oai_citation:46‡arxiv.org](https://arxiv.org/abs/2311.10538#:~:text=wild.%20Yet%20real,that%20is%20flexible%20enough%20to). For instance, if multiple questionable steps led up to the failure, those are all recorded. This allows a developer or safety engineer to review the transcript and understand how the agent got to that point. In their real-world tests on AutoGPT, this monitoring uncovered various failure patterns and challenges (which were then used to refine the safety rules) [oai_citation:47‡arxiv.org](https://arxiv.org/abs/2311.10538#:~:text=monitor%20existing%20LLM%20agents%2C%20and%2C,autonomous%20agents%20grow%20more%20capable). The design demonstrates a **deterministic supervisor** pattern: the monitor has final authority and is not steerable by the agent. If a condition is met (unsafe action identified), the agent *cannot override or negotiate* its way out – it gets stopped and flagged. This kind of external interrupter is important for high-stakes deployments, ensuring there is always a layer that the agent does not control (often termed a “red button” in AI safety discourse). AgentMonitor’s simplicity – it can wrap around *any* LLM agent by observing its I/O – makes it a promising blueprint for practical oversight tools.

### Tool Use Policies and Emulation: *Other Notable Approaches*

Beyond the above, several other approaches in literature and open-source address safe tool use and planning:

- **InjecAgent (2024)** explores *indirect prompt injection attacks* on agents [oai_citation:48‡aclanthology.org](https://aclanthology.org/anthology-files/anthology-files/pdf/findings/2024.findings-emnlp.79.pdf#:~:text=encounter%20on%20the%20open%20internet,turn%20interactions). It demonstrates how an attacker can embed malicious instructions in a website or other content that an agent might read, causing the agent to execute harmful actions unwittingly [oai_citation:49‡aclanthology.org](https://aclanthology.org/anthology-files/anthology-files/pdf/findings/2024.findings-emnlp.79.pdf#:~:text=encounter%20on%20the%20open%20internet,turn%20interactions). While primarily an attack demonstration, it underscores the need for agents to have input sanitization and validation (e.g. an agent’s tool that reads web content should strip or neutralize hidden commands).

- **Constitutional AI for Agents:** Anthropic’s *Constitutional AI* approach (earlier used to align chat models) has inspired methods to give agents an internalized set of rules that they *self-enforce* via reflection. This is conceptually similar to TrustAgent’s constitution. For instance, an agent might have a second “reflection phase” after generating a draft action where it asks, “Is this action consistent with my safety principles?” and if not, revises it. This is a kind of *self-supervision* that can complement external monitors.

- **Plan Verification and Sandboxing:** Some works propose running a **sandboxed trial of an agent’s plan** before committing to it. For example, an agent might generate a sequence of tool calls to accomplish a task, and a separate process (or the agent itself in a different mode) simulates that plan step by step in a controlled environment to check outcomes. This is akin to ToolEmu but could be done with actual isolated sub-processes. If the sandboxed execution shows any dangerous side-effect (e.g. the plan would delete a file or send out data), the real execution is blocked. This *safe planning* approach treats the plan as code that must pass a series of unit tests or safety checks.

- **Auditing Memory and Knowledge:** Since LLM agents with long-term memory could accumulate potentially sensitive or corrupted information, some frameworks log all updates to an agent’s knowledge base. For instance, if an agent saves a piece of data from a tool result (like the content of an email or a file), the system might tag it and ensure any future use of it is policy-compliant (no sharing with unauthorized parties, etc.). Logging memory accesses and modifications helps in post-hoc analysis of any leak—one can trace *which* tool output led to a forbidden piece of data being revealed.

Across these frameworks, a recurring pattern is **intercepting tool calls via a mediator**. Rather than an agent calling system tools or APIs directly, calls are funneled through a *policy enforcement layer*. This layer can be as simple as a whitelist filter (e.g. only allow `read_file()` on certain directories) or as complex as an LLM-based emulator or monitor that decides dynamically. By not letting the agent execute actions directly, the system gains an opportunity to *inspect and vet* each action against the threat model.

## Industry Practices: AI Sandboxes and Operational Sandboxing

In industry, the concept of an “**AI sandbox**” has gained traction as companies deploy LLM-powered features. An AI sandbox is typically a **controlled, isolated environment** in which an LLM agent (or any code it produces) can run without endangering production systems or sensitive data [oai_citation:50‡research.aimultiple.com](https://research.aimultiple.com/ai-sandbox/#:~:text=Examples%20research,risking%20real%20systems%20or) [oai_citation:51‡medium.com](https://medium.com/@safelishare/safelishare-ai-sandbox-supplement-large-language-models-with-internal-data-9f1d42281552#:~:text=In%20the%20rapidly%20evolving%20landscape,both%20training%20and%20inference%20processes). The philosophy is to contain the agent’s impact the same way a browser sandbox contains web page scripts.

A prime example is OpenAI’s approach to tools like the Code Interpreter (now called *Advanced Data Analysis*) and Plugins for ChatGPT. When users enable these, OpenAI actually spins up a **secure container** (a temporary Debian-based Linux sandbox) in which the agent’s code execution or tool use occurs [oai_citation:52‡0din.ai](https://0din.ai/blog/prompt-injecting-your-way-to-shell-openai-s-containerized-chatgpt-environment#:~:text=Sandbox%20Environment%20Insights%3A%20It%20dives,structures%20and%20enable%20file%20management) [oai_citation:53‡0din.ai](https://0din.ai/blog/prompt-injecting-your-way-to-shell-openai-s-containerized-chatgpt-environment#:~:text=locally%20on%20OpenAI%27s%20Docker%20instance%3F,how%20I%20interact%20with%20ChatGPT). This sandbox has a **controlled file system and process space** [oai_citation:54‡0din.ai](https://0din.ai/blog/prompt-injecting-your-way-to-shell-openai-s-containerized-chatgpt-environment#:~:text=Sandbox%20Environment%20Insights%3A%20It%20dives,structures%20and%20enable%20file%20management). For instance, the Code Interpreter’s container only has access to a working directory and some allowed libraries – it cannot read arbitrary system files or make uncontrolled outbound network calls [oai_citation:55‡community.openai.com](https://community.openai.com/t/is-this-a-server-leakage-can-anyone-evaluate-the-risk/742973#:~:text=Is%20this%20a%20server%20leakage%3F,My) [oai_citation:56‡medium.com](https://medium.com/@safelishare/safelishare-ai-sandbox-supplement-large-language-models-with-internal-data-9f1d42281552#:~:text=,blocked%2C%20preventing%20unauthorized%20data%20leakage). In fact, **outbound networking is usually completely blocked** in these sandboxes to prevent data exfiltration [oai_citation:57‡medium.com](https://medium.com/@safelishare/safelishare-ai-sandbox-supplement-large-language-models-with-internal-data-9f1d42281552#:~:text=,blocked%2C%20preventing%20unauthorized%20data%20leakage). The container is also ephemeral: when the session ends, the entire filesystem is wiped, ensuring that no persistent state can leak between users or sessions. This addresses multi-tenant security concerns where one user’s data could otherwise be left for the next – in the sandbox, *each session is fresh and isolated*.

**Threat model:** Industry sandboxes assume the agent *will* try to do arbitrary things (since users might prompt it to). Thus, they guard the operating system and network. Common threat examples include: an agent might attempt to read system files (like `/etc/passwd` on a Unix system) or connect to external sites to send data. Without sandboxing, an agent with code execution could indeed perform such actions. With sandboxing, these actions either fail or are limited to harmless scope. For example, if the agent tries `os.listdir("/")` in a sandboxed Python environment, it will only see the container’s limited root, not the host filesystem (and often that container root has nothing sensitive in it) [oai_citation:58‡medium.com](https://medium.com/@akanksha.amarendra6/inadequate-sandboxing-in-llms-owasp-llm-top-10-45be4c88c402#:~:text=For%20example%2C%20an%20LLM%20agent,in%20the%20system%E2%80%9D%20might%20run). If it tries to open a socket or an HTTP request, the network rules can prevent any real connection.

**Intercepting and filtering calls:** In operational sandboxes, interception is often done at the system-call or API level. For instance, an agent’s call to open a file first goes through the OS, which will enforce file permissions and mount constraints. Cloud providers and products sometimes use *seccomp filters* or library overrides to disallow dangerous syscalls (like `mount`, `kill`, or raw socket creation) – indeed, users found that ChatGPT’s sandbox would refuse certain operations (`mount` was explicitly blocked) [oai_citation:59‡community.openai.com](https://community.openai.com/t/is-this-a-server-leakage-can-anyone-evaluate-the-risk/742973#:~:text=Is%20this%20a%20server%20leakage%3F,My). Network calls are commonly disabled by not provisioning any network interface in the container, or by firewall rules that whitelist only certain domains. Some sandboxed agents allow controlled internet access via proxy APIs that filter content (e.g. only allowing calls to known-safe endpoints or stripping certain HTML/JS when browsing).

Beyond OS-level sandboxing, **industry practice includes policy layers** similar to academic ideas. OpenAI’s plugins, for example, come with a *manifest* that declares what the plugin can do, and the model is bounded by those. If a plugin function is defined to only fetch weather data, the model shouldn’t be able to call arbitrary code through it. Robust implementations validate the parameters server-side – e.g., if there’s a function `read_file(path)` exposed to the LLM, the plugin backend will check that `path` is in an allowed directory and not something like `/etc/passwd` [oai_citation:60‡medium.com](https://medium.com/@akanksha.amarendra6/inadequate-sandboxing-in-llms-owasp-llm-top-10-45be4c88c402#:~:text=3,Calls). This prevents the LLM from exploiting a function beyond its intended scope. Similarly, cloud providers recommend using **“approved tool lists”** and **API gateways** for any external actions an agent can take. The agent might “think” it has a full shell, but in reality the shell commands it issues are parsed by a mediator service that only executes whitelisted commands with sanitized arguments.

**Supervisors and policy engines in practice:** Many companies layer an *AI policy engine* that scans prompts and outputs. For instance, Anthropic and OpenAI have *content moderation filters* running on model outputs to catch disallowed content (hate, self-harm, etc.) as a last line of defense. These filters are typically not part of the model prompt (so the agent cannot see or manipulate them); instead, they operate as an external service that receives the final text and returns a compliance judgment. If the text trips a wire (say it contains an SSN or a profanity), the system may refuse to output it to the user or replace it with a safe message. We will discuss such outbound filters in the next section.

Another practice is requiring **user confirmation for high-risk actions**. For example, if an agent is about to perform an irreversible action (like sending an email or making a purchase), the system might pause and ask the human “The agent wants to do X – do you allow this?” behind the scenes. This is a straightforward policy: only low-risk tool calls are auto-executed, others need explicit human go-ahead. The agent cannot easily circumvent this because it doesn’t control the interface that asks the user.

**Logging and auditing** are critical in enterprise settings. All tool usage by an agent is usually logged (which function was called, with what arguments, and when). If the agent runs code, the code and its outputs are captured. Many sandbox environments also log system-level events, such as file accesses or network requests (attempted or successful). These logs serve both for security auditing and for debugging when the agent misbehaves. For example, if an agent wiped a directory it shouldn’t have, the logs would show the sequence of prompts and actions that led to that, helping developers patch the gap. Some organizations integrate these logs with enterprise security monitoring and alerting infrastructure [oai_citation:61‡medium.com](https://medium.com/@safelishare/safelishare-ai-sandbox-supplement-large-language-models-with-internal-data-9f1d42281552#:~:text=Enterprise), so any anomalous agent behavior triggers alerts to security teams.

It’s worth noting that **confidential computing** is being explored for AI sandboxes [oai_citation:62‡medium.com](https://medium.com/@safelishare/safelishare-ai-sandbox-supplement-large-language-models-with-internal-data-9f1d42281552#:~:text=Privacy). This means using hardware-secured enclaves (like Intel SGX or AMD SEV) to run the LLM and its tools so that even if the host system is compromised, the data and code inside the enclave remain protected. SafeLiShare’s AI Sandbox (2024) is one such solution, which *verifies models inside secure enclaves and blocks any unauthorized data egress* [oai_citation:63‡medium.com](https://medium.com/@safelishare/safelishare-ai-sandbox-supplement-large-language-models-with-internal-data-9f1d42281552#:~:text=,blocked%2C%20preventing%20unauthorized%20data%20leakage) [oai_citation:64‡medium.com](https://medium.com/@safelishare/safelishare-ai-sandbox-supplement-large-language-models-with-internal-data-9f1d42281552#:~:text=,of%20model%20development%20and%20deployment). The enclave approach primarily addresses privacy (protecting sensitive data from leaking out or being inspected), but it also adds another layer of integrity – the model can be checked for tampering via attestation before it runs.

In summary, industry sandboxes implement *defense-in-depth*: OS-level isolation, network restrictions, strict tool APIs, external approval for critical actions, and exhaustive logging. The goal is an environment where even if the LLM agent goes rogue (or is tricked by input), the damage is confined to a sandbox that can be shut down and inspected [oai_citation:65‡0din.ai](https://0din.ai/blog/prompt-injecting-your-way-to-shell-openai-s-containerized-chatgpt-environment#:~:text=Exploring%20the%20Limits%3A%20This%20blog,underlying%20structure%20in%20unexpected%20ways) [oai_citation:66‡medium.com](https://medium.com/@akanksha.amarendra6/inadequate-sandboxing-in-llms-owasp-llm-top-10-45be4c88c402#:~:text=from%20the%20underlying%20system%20or,a%20model%20or%20agent%20from). As one expert guide neatly put it, *sandboxing should be a first-class security concern — not an afterthought* [oai_citation:67‡medium.com](https://medium.com/@akanksha.amarendra6/inadequate-sandboxing-in-llms-owasp-llm-top-10-45be4c88c402#:~:text=LLMs%20are%20powerful%2C%20but%20that,to%20prevent%20misuse%20or%20compromise) for any LLM integration.

## Design Patterns for Safe LLM Agent Deployment

Drawing on the above, we conclude with concrete **design patterns** that one can apply when building a local LLM-based assistant (for example, a Mac-based personal GPT agent) with safety in mind. These patterns address the key areas of concern: controlling the agent’s ability to run unchecked, filtering its outputs, and isolating its execution environment.

### 1. Deterministic Supervisor with a “Kill Switch”

Implement a supervisory process that **monitors the agent’s actions in real time and can deterministically terminate the agent** if certain conditions are met. This supervisor should be external to the agent (not influenced by the agent’s prompts or internal logic) – for instance, a separate thread or service that intercepts all tool/API calls from the agent. Define a set of *non-negotiable rules* (derived from your threat model) that, when violated, immediately trigger a stop. Examples of such rules: attempting to modify or delete files outside an allowed directory, attempting to execute a disallowed system command (like launching an unknown executable or calling sensitive OS APIs), or trying to send data to an external network without permission. Because this supervisor is deterministic and rule-based, the agent **cannot “talk its way out”** of a shutdown. This is analogous to AgentMonitor’s approach which *halts the entire agent test when an unsafe action is detected* [oai_citation:68‡arxiv.org](https://arxiv.org/abs/2311.10538#:~:text=wild.%20Yet%20real,an%20adversarial%20simulated%20agent%2C%20we).

A practical way to implement this on a Mac could be to run the agent as a child process of a controller. The controller intercepts system calls or high-level tool invocations using wrappers. For example, if the agent wants to run a shell command, it must call a function exposed by the controller – something like `supervisor.run_shell(cmd)`. The supervisor function checks `cmd` against a safe list (no destructive commands, no elevation to sudo, etc.). If the check fails, it can terminate the agent process immediately and log the incident. The supervisor can also monitor resource usage and infinite loops: using OS signals or a watchdog timer, it can kill the agent if it runs too long or consumes too much CPU, preventing runaway scenarios. This deterministic supervisor acts as the **ultimate safety net**, analogous to a circuit breaker. It should also produce an **audit log** whenever it intervenes – e.g., “Agent killed at 3:42PM for attempting forbidden action: `<details>`,” enabling developers or users to review what went wrong.

### 2. Outbound Response Policy Filter (Not Promptable by the Agent)

It’s essential to control what the agent ultimately communicates to the user or external systems. An **outbound-text policy filter** examines the agent’s proposed output **after** the agent generates it, and will censor or modify any content that violates guidelines. The key is that this filter is *not part of the agent’s prompt* and cannot be influenced by the agent during generation. In other words, it sits **post-generation, pre-delivery**. A proven implementation is the “**supervisor LLM**” technique [oai_citation:69‡telusdigital.com](https://www.telusdigital.com/insights/data-and-ai/article/llm-moderation-supervisor#:~:text=The%20%E2%80%9Csupervisor%E2%80%9D%20is%20an%20LLM,guidelines%20provided%20via%20a%20prompt): use a separate, locked-down language model that takes the agent’s output and a fixed set of policy rules as input, and simply returns whether the content is compliant or not. Because this supervisor model only sees the final output (not the original user prompt or the agent’s chain-of-thought), it’s much harder for an attacker or the agent itself to perform a prompt injection on the filter [oai_citation:70‡telusdigital.com](https://www.telusdigital.com/insights/data-and-ai/article/llm-moderation-supervisor#:~:text=When%20we%20first%20came%20up,it%20wrongly%20classifies%20the%20input). As Zakey Faieq *et al.* found, if the filter model is asked to judge the *agent’s output* rather than the *user’s potentially malicious input*, it is **“far less likely to be jailbroken”** by clever prompt attacks [oai_citation:71‡telusdigital.com](https://www.telusdigital.com/insights/data-and-ai/article/llm-moderation-supervisor#:~:text=When%20we%20first%20came%20up,it%20wrongly%20classifies%20the%20input). The filter’s instructions (the content policy) are given in its own system prompt that the agent cannot see or change.

For example, you might have a policy: *“No disclosure of personal identifiable information (PII), no hate speech, no instructions for illegal activities.”* After your agent produces a response, you send `filter_model("OUTPUT: [agent's answer]\nDoes this output follow all policies? Yes/No")`. If it says “No” (meaning it found a violation), your system can automatically replace the agent’s output with a safe fallback (like “I’m sorry, I cannot share that result”) or a sanitized version. Some implementations simply never show the unsafe output to the user at all [oai_citation:72‡telusdigital.com](https://www.telusdigital.com/insights/data-and-ai/article/llm-moderation-supervisor#:~:text=When%20the%20supervisor%20LLM%20finds,a%20noncompliant%20response). This filter approach is flexible: updating policies is as easy as changing the filter prompt, and you don’t need to hard-code countless rules or gather a classifier training set [oai_citation:73‡telusdigital.com](https://www.telusdigital.com/insights/data-and-ai/article/llm-moderation-supervisor#:~:text=Furthermore%2C%20LLMs%20excel%20when%20given,for%20our%20clients%E2%80%99%20use%20cases).

The filter can also be **deterministic or hybrid**. Instead of an LLM, one could use a regex or list-based filter for certain obvious cases (curse words, known sensitive keywords) which is very deterministic. However, learning-based filters (like an LLM or a fine-tuned classifier) are useful for subtle or context-dependent policies. In either case, the crucial aspect is the separation of concerns: the agent generates freely, then the filter acts as a **gatekeeper**. The agent never gets to see the “gate’s” internal logic, so it can’t directly manipulate it. Logging here is again important: whenever a response is blocked or altered, record what rule was triggered. Over time, those logs might show patterns (e.g., the agent frequently attempts to output a certain disallowed phrase), which can inform adjusting the agent’s prompts or improving its training to naturally avoid those outputs.

### 3. Local Operational Sandbox (Least-Privilege Environment)

When deploying an LLM agent on your local machine (e.g., a Mac-based assistant), treat it as potentially unsafe code by default – **run it with the principle of least privilege**. On a Mac, one straightforward method is to create a separate macOS user account (with no admin rights) for the agent. Run the agent process under this account so that it cannot access your personal files or sensitive directories by OS permission. Next, restrict the environment further: for instance, you could use containerization (Docker on Mac, or a lightweight VM) to give the agent a **minimal filesystem** – only a working folder and necessary executables. Mount only specific directories (if the agent needs to read some data files, mount those alone, and mount them read-only if the agent only needs to read) [oai_citation:74‡medium.com](https://medium.com/@akanksha.amarendra6/inadequate-sandboxing-in-llms-owasp-llm-top-10-45be4c88c402#:~:text=,only%20permissions). Ensure that no authentication credentials or secrets are available in its environment variables. Disable network access unless absolutely needed: if the agent must access the internet, route it through a proxy that you control, where you can filter which URLs it’s allowed to reach. Many personal assistants might not need internet at all – in which case, simply do not grant them any network interface (for example, run Docker with the `--network none` option, or use the Mac’s Application Sandbox to block outgoing connections) [oai_citation:75‡medium.com](https://medium.com/@safelishare/safelishare-ai-sandbox-supplement-large-language-models-with-internal-data-9f1d42281552#:~:text=,blocked%2C%20preventing%20unauthorized%20data%20leakage).

Additionally, **whitelist the tools** and system commands the agent can use. If your agent uses Python, consider using a sandboxed Python interpreter. This could mean enabling Python’s own restrictions (like a sandboxed `exec` environment where dangerous builtins are removed) or using a tool like *JailPython*. More practically, you can intercept imports and restrict modules: e.g., allow `math` and `json`, but forbid `os`, `subprocess`, or `shutil` unless you wrap them [oai_citation:76‡medium.com](https://medium.com/@akanksha.amarendra6/inadequate-sandboxing-in-llms-owasp-llm-top-10-45be4c88c402#:~:text=1) [oai_citation:77‡medium.com](https://medium.com/@akanksha.amarendra6/inadequate-sandboxing-in-llms-owasp-llm-top-10-45be4c88c402#:~:text=execution%2C%20often%20called%20a%20Python,subprocess). If the agent uses shell commands via a Python REPL, you might implement a custom `os.system` that checks the command string against rules. Similarly, if using AppleScript or GUI automation, run those with minimal privileges and confirmations.

The sandbox should also enforce **resource limits**. Using macOS built-in mechanisms or container settings, cap the CPU and memory the agent can use, to prevent it from hogging the system or fork-bombing. Timeouts are another tool: any tool invocation or code execution the agent does should have a reasonable timeout (few seconds, perhaps), enforced by the supervisor. This prevents the agent from, say, accidentally entering an infinite loop that never relinquishes control [oai_citation:78‡medium.com](https://medium.com/@akanksha.amarendra6/inadequate-sandboxing-in-llms-owasp-llm-top-10-45be4c88c402#:~:text=Code%20Execution%20Hardening).

Crucially, **monitor and log everything within the sandbox**. As the OWASP LLM safety guidelines note, developers should *“capture logs of all tool invocations and system calls”* and *“alert on suspicious behavior, like attempts to access protected files”* [oai_citation:79‡medium.com](https://medium.com/@akanksha.amarendra6/inadequate-sandboxing-in-llms-owasp-llm-top-10-45be4c88c402#:~:text=Monitoring%20and%20Logging). On a Mac, you could leverage the system’s audit facility or simple logging in your wrapper functions. For example, log every file path the agent tries to open, and if it’s outside the allowed scope, not only block it but record an alert. Logging network attempts (destinations, ports) is similarly useful. Over time, these audit logs help in both security (detecting an intrusion or misuse attempt) and in improving the agent (maybe the agent repeatedly tries to use a tool it shouldn’t, indicating it needs better instructions).

In essence, the **operational sandbox** for a local assistant is about layering the environment defenses such that even if the agent goes off-script, it *runs into walls*. A container or restricted user account ensures it cannot impact the host system beyond its little sandbox. Within that, careful scoping of file and network access stops data leaks and destructive actions [oai_citation:80‡medium.com](https://medium.com/@akanksha.amarendra6/inadequate-sandboxing-in-llms-owasp-llm-top-10-45be4c88c402#:~:text=from%20the%20underlying%20system%20or,a%20model%20or%20agent%20from) [oai_citation:81‡medium.com](https://medium.com/@akanksha.amarendra6/inadequate-sandboxing-in-llms-owasp-llm-top-10-45be4c88c402#:~:text=,Leaking%20data%20across%20user%20sessions). And continuous monitoring provides a chance to terminate processes that start misbehaving. By adopting such a sandbox, you dramatically reduce the potential harm from running an LLM agent on your machine – it’s the same idea as running unknown code in a VM for safety.

---

By combining these patterns – an **external kill-switch supervisor**, a **robust output filter**, and a **locked-down sandboxed environment** – one can **adapt the latest safety research into practice** for LLM-based agents. These measures correspond to multiple layers of defense: **preventive** (constraining what the agent can do or see), **detective** (monitoring and filtering its decisions), and **responsive** (interrupting and logging on any violation). While no single solution is foolproof against all threats, together they greatly mitigate the risks. An LLM agent built with these guardrails can be a much more trustworthy assistant, **staying within approved bounds** and allowing developers and users to harness its capabilities with peace of mind [oai_citation:82‡medium.com](https://medium.com/@akanksha.amarendra6/inadequate-sandboxing-in-llms-owasp-llm-top-10-45be4c88c402#:~:text=LLMs%20are%20powerful%2C%20but%20that,to%20prevent%20misuse%20or%20compromise).

**Sources:**

- Yu *et al.*, *“A Survey on Trustworthy LLM Agents: Threats and Countermeasures (TrustAgent)”*, arXiv 2024.  [oai_citation:83‡github.com](https://github.com/Ymm-cll/TrustAgent#:~:text=,leakage%2C%20hallucinations%2C%20and%20fairness%20biases) [oai_citation:84‡emergentmind.com](https://www.emergentmind.com/papers/2402.01586#:~:text=integrated%20into%20high,agent%27s%20safety%20across%20multiple%20domains)

- Ruan *et al.*, *“Identifying the Risks of LM Agents with an LM-Emulated Sandbox (ToolEmu)”*, arXiv 2024.  [oai_citation:85‡arxiv.org](https://arxiv.org/abs/2309.15817#:~:text=difficult%20to%20find%20high,we%20provide%20a%20quantitative%20risk) [oai_citation:86‡arxiv.org](https://arxiv.org/abs/2309.15817#:~:text=enables%20the%20testing%20of%20LM,9)

- Naihin *et al.*, *“Testing Language Model Agents Safely in the Wild (AgentMonitor)”*, arXiv 2023.  [oai_citation:87‡arxiv.org](https://arxiv.org/abs/2311.10538#:~:text=,that%20is%20flexible%20enough%20to) [oai_citation:88‡arxiv.org](https://arxiv.org/abs/2311.10538#:~:text=autonomous%20agent%20tests%20on%20the,autonomous%20agents%20grow%20more%20capable)

- Faieq *et al.*, *“Using LLMs to Moderate LLMs: The Supervisor Technique”*, TELUS Digital blog 2024.  [oai_citation:89‡telusdigital.com](https://www.telusdigital.com/insights/data-and-ai/article/llm-moderation-supervisor#:~:text=The%20%E2%80%9Csupervisor%E2%80%9D%20is%20an%20LLM,guidelines%20provided%20via%20a%20prompt) [oai_citation:90‡telusdigital.com](https://www.telusdigital.com/insights/data-and-ai/article/llm-moderation-supervisor#:~:text=However%2C%20we%20found%20that%20by,less%20likely%20to%20be%20jailbroken)

- SafeLiShare, *“AI Sandbox: Supplement LLMs with Internal Data”*, *Medium* 2024.  [oai_citation:91‡medium.com](https://medium.com/@safelishare/safelishare-ai-sandbox-supplement-large-language-models-with-internal-data-9f1d42281552#:~:text=,blocked%2C%20preventing%20unauthorized%20data%20leakage) [oai_citation:92‡medium.com](https://medium.com/@safelishare/safelishare-ai-sandbox-supplement-large-language-models-with-internal-data-9f1d42281552#:~:text=,of%20model%20development%20and%20deployment)

- Akanksha, *“Inadequate Sandboxing in LLMs — OWASP Top 10”*, *Medium* 2025.  [oai_citation:93‡medium.com](https://medium.com/@akanksha.amarendra6/inadequate-sandboxing-in-llms-owasp-llm-top-10-45be4c88c402#:~:text=from%20the%20underlying%20system%20or,a%20model%20or%20agent%20from) [oai_citation:94‡medium.com](https://medium.com/@akanksha.amarendra6/inadequate-sandboxing-in-llms-owasp-llm-top-10-45be4c88c402#:~:text=Environment%20Isolation) [oai_citation:95‡medium.com](https://medium.com/@akanksha.amarendra6/inadequate-sandboxing-in-llms-owasp-llm-top-10-45be4c88c402#:~:text=Monitoring%20and%20Logging)

- Tang *et al.*, *“Security of LLM-based Agents: A Comprehensive Survey”*, Info Fusion 2025.  [oai_citation:96‡sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S1566253525010036#:~:text=However%2C%20the%20increasing%20complexity%20and,broadens%20further%20and%20undergoes%20a)
