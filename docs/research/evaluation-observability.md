# Evaluating and Observing Agentic AI Systems (2023–2025)

## Agent Evaluation Frameworks (Academic Proposals)

Academic work in 2023–2025 emphasizes that evaluating **LLM-based agents** requires multi-dimensional metrics beyond those for static LLMs [oai_citation:0‡arxiv.org](https://arxiv.org/html/2507.21504v1#:~:text=analogy%2C%20LLM%20evaluation%20is%20like,as%20under%20various%20driving%20conditions) [oai_citation:1‡getmaxim.ai](https://www.getmaxim.ai/blog/llm-agent-evaluation-framework-comparison/#:~:text=Multi). Researchers propose frameworks and benchmarks that assess agents on several axes:

- **Task Success (Completion Rate):** Does the agent achieve the end goal or complete the task? This is often a binary success/failure metric or a success rate across tasks [oai_citation:2‡getmaxim.ai](https://www.getmaxim.ai/blog/llm-agent-evaluation-framework-comparison/#:~:text=AgentBench%20,shopping%2C%20database%20operations%2C%20and%20coding) [oai_citation:3‡getmaxim.ai](https://www.getmaxim.ai/blog/llm-agent-evaluation-framework-comparison/#:~:text=ToolBench). For example, AgentBench (2023) measures whether agents can finish tasks across diverse domains like web browsing, database queries, or coding challenges [oai_citation:4‡getmaxim.ai](https://www.getmaxim.ai/blog/llm-agent-evaluation-framework-comparison/#:~:text=AgentBench). Task completion focuses on outcomes, i.e. whether the agent’s final action or answer satisfies the user’s request [oai_citation:5‡arxiv.org](https://arxiv.org/html/2507.21504v1#:~:text=category%20encompasses%20aspects%20such%20as,output%20quality%2C%20latency%2C%20and%20cost).
- **Output Quality:** Even if a task is completed, the *quality* of the final response matters. This includes correctness/accuracy of information, relevance to the query, clarity and coherence of the answer, and usefulness to the user [oai_citation:6‡arxiv.org](https://arxiv.org/html/2507.21504v1#:~:text=) [oai_citation:7‡arxiv.org](https://arxiv.org/html/2507.21504v1#:~:text=match%20at%20L547%20Output%20Quality,Shi%20et%C2%A0al). Some academic evaluations use multiple criteria here – e.g. scoring an agent’s answer for factual accuracy, logical coherence, and user satisfaction separately [oai_citation:8‡arxiv.org](https://arxiv.org/html/2507.21504v1#:~:text=match%20at%20L547%20Output%20Quality,Shi%20et%C2%A0al). Often human judges or **LLM-as-judge** techniques (see below) are used to rate output quality when no single ground-truth answer exists.
- **Latency and Cost:** Agents may call external tools or perform multi-step reasoning, so **timing** and efficiency are tracked. Researchers note that metrics like end-to-end latency, number of reasoning steps, and token/call usage cost are important for practical deployment [oai_citation:9‡arxiv.org](https://arxiv.org/html/2507.21504v1#:~:text=1,Agent%20Collaboration) [oai_citation:10‡langfuse.com](https://langfuse.com/blog/2024-10-opentelemetry-for-llm-observability#:~:text=LLM%20applications%20present%20distinct%20challenges,they%20are%20in%20traditional%20applications). For instance, a framework might record how long each tool invocation takes and the total time to complete the task, as well as the API or compute cost incurred [oai_citation:11‡langfuse.com](https://langfuse.com/blog/2024-10-opentelemetry-for-llm-observability#:~:text=LLM%20applications%20present%20distinct%20challenges,they%20are%20in%20traditional%20applications).
- **Tool Use and Planning:** A defining feature of agentic systems is their use of tools/APIs and multi-step plans. Thus, academic proposals evaluate **tool-use accuracy** – whether the agent chose appropriate tools and used them with correct arguments [oai_citation:12‡docs.nvidia.com](https://docs.nvidia.com/nemo/microservices/latest/evaluate/flows/agentic.html#:~:text=1,intermediate%20steps%20during%20agent%20execution) [oai_citation:13‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=,step%20workflows) – and **planning/problem-solving** – whether the action sequence was effective and minimal. *ToolBench* (2023) exemplifies this by specifically benchmarking how well agents select and invoke tools via a standardized API across tasks [oai_citation:14‡getmaxim.ai](https://www.getmaxim.ai/blog/llm-agent-evaluation-framework-comparison/#:~:text=ToolBench). Metrics here include the correctness of each tool call (did the agent call the right function with the right parameters at the right time) and possibly counts of unnecessary or failed tool calls [oai_citation:15‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=,aligned%20%3F%201%20%3A%200) [oai_citation:16‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=1,right%20tool%20from%20many%20options).
- **Reasoning Trace & Memory:** Beyond final outputs, researchers stress *process-oriented* evaluation [oai_citation:17‡getmaxim.ai](https://www.getmaxim.ai/blog/llm-agent-evaluation-framework-comparison/#:~:text=Process). This means examining the agent’s reasoning trace or chain-of-thought quality (logical correctness of intermediate steps) and its use of memory/context over long interactions [oai_citation:18‡getmaxim.ai](https://www.getmaxim.ai/blog/llm-agent-evaluation-framework-comparison/#:~:text=and%20APIs%20%5B9%5D%5B10%5D%5B11%5D.%20%2A%20Self,14). While harder to quantify, some frameworks rate the quality of intermediate reasoning or check for consistency in the agent’s plan. For example, does the agent self-correct when off track? Does it remember crucial info provided earlier in the conversation? These might be evaluated via scenario-based tests or by expert review of the agent’s thought process [oai_citation:19‡getmaxim.ai](https://www.getmaxim.ai/blog/llm-agent-evaluation-framework-comparison/#:~:text=Process).
- **Reliability and Robustness:** Agents are tested for **consistency** (producing stable results given the same query or small variations) and **robustness** to perturbations [oai_citation:20‡arxiv.org](https://arxiv.org/html/2507.21504v1#:~:text=4.%203.2.4%20Multi,4%20Evaluation%20Process). For instance, an evaluation might perturb the input or environment and see if the agent still succeeds [oai_citation:21‡arxiv.org](https://arxiv.org/html/2507.21504v1#:~:text=match%20at%20L431%20and%20measuring,performance%20degrades%20under%20input%20variation). Stress tests (e.g. malformed inputs, novel situations) gauge whether the agent can handle edge cases without failure.
- **Safety and Alignment:** Ensuring agentic AI behaves safely is critical. Academic proposals include metrics for **safety compliance**, such as avoiding toxic or biased outputs, respecting usage policies, and not leaking private data [oai_citation:22‡arxiv.org](https://arxiv.org/html/2507.21504v1#:~:text=1,18). In evaluations, this can involve red-teaming prompts or checking if the agent refuses disallowed requests. For example, an agent might be scored on whether it refrains from giving harmful advice or making biased statements when provoked [oai_citation:23‡arxiv.org](https://arxiv.org/html/2507.21504v1#:~:text=1,18). Some benchmarks explicitly incorporate tests for fairness, toxicity, or privacy compliance as part of the evaluation suite [oai_citation:24‡arxiv.org](https://arxiv.org/html/2507.21504v1#:~:text=1,18). Safety is often measured by counting policy violations or using classification models/humans to judge if an output crosses certain risk thresholds.

**Framework examples:** The **Survey of LLM Agent Evaluation** (Mohammadi et al., 2025) provides a taxonomy covering many of the above dimensions [oai_citation:25‡arxiv.org](https://arxiv.org/html/2507.21504v1#:~:text=evaluating%20these%20agents%20remains%20a,the%20future%20research%20directions%2C%20including) [oai_citation:26‡arxiv.org](https://arxiv.org/html/2507.21504v1#:~:text=oriented%20aspects%20such%20as%20task,agent%20collaboration.%20These%20capabilities). Specific benchmarks illustrate what is measured: *AgentBench* stresses diverse task success [oai_citation:27‡getmaxim.ai](https://www.getmaxim.ai/blog/llm-agent-evaluation-framework-comparison/#:~:text=AgentBench), *ToolBench* focuses on tool-use correctness [oai_citation:28‡getmaxim.ai](https://www.getmaxim.ai/blog/llm-agent-evaluation-framework-comparison/#:~:text=ToolBench), *WebArena* evaluates success in web tasks (with timing of interactions) [oai_citation:29‡getmaxim.ai](https://www.getmaxim.ai/blog/llm-agent-evaluation-framework-comparison/#:~:text=WebArena), and others like GAIA test decision-making in simulated game environments (long horizon planning and adaptability) [oai_citation:30‡getmaxim.ai](https://www.getmaxim.ai/blog/llm-agent-evaluation-framework-comparison/#:~:text=GAIA). A common theme is that **multi-dimensional scoring** is needed – simple accuracy or success/fail metrics alone are insufficient for complex agents [oai_citation:31‡getmaxim.ai](https://www.getmaxim.ai/blog/llm-agent-evaluation-framework-comparison/#:~:text=Multi). Instead, researchers propose evaluating agents along **multiple axes concurrently**, e.g. task completion rate, efficiency (time or steps taken), correctness of process, and safety compliance [oai_citation:32‡getmaxim.ai](https://www.getmaxim.ai/blog/llm-agent-evaluation-framework-comparison/#:~:text=Multi). This holistic approach ensures an agent that “succeeds” by reaching an answer but, say, uses 100 steps or violates a rule would be recognized as underperforming on other metrics.

## Industry Approaches to Agent Observability

Outside academia, there is a strong focus on **observability and tracing** for agentic AI systems. Industry tools and blogs emphasize capturing detailed traces of an agent’s execution to enable measurement and debugging in real time. Key aspects of these observability solutions include:

- **Structured Traces & Spans:** An agent’s entire interaction (from user query to final response with all intermediate tool calls) is treated as a **trace** composed of hierarchical *spans*. Each span represents a step or component: e.g. receiving the user query, calling an LLM model, invoking a tool, getting tool result, responding to user [oai_citation:33‡getsnippets.ai](https://www.getsnippets.ai/articles/snippets-ai-vs-langfuse-vs-opentelemetry#:~:text=Langfuse%20treats%20every%20LLM%20interaction,each%20as%20a%20clickable%20span). Tools like **Langfuse** provide this out-of-the-box: *“Langfuse treats every LLM interaction like a story with chapters… it logs the full narrative: user input, retrieval step, model call, tool use, final output – each as a clickable span.”* [oai_citation:34‡getsnippets.ai](https://www.getsnippets.ai/articles/snippets-ai-vs-langfuse-vs-opentelemetry#:~:text=Langfuse%20treats%20every%20LLM%20interaction,each%20as%20a%20clickable%20span). This means you can inspect each agent action in sequence, along with timing and data. For example, one span might be a “search documents” tool call – you’d see its start/end timestamps, the query sent, and the returned result, followed by the next span for the LLM using that info. Structuring traces this way allows **process evaluation** in production: you can see where an agent went wrong (e.g. took a wrong tool path or got stuck looping).
- **Captured Metadata & Metrics:** Along with the trace structure, observability frameworks record rich metadata on each span. This often includes **latency**, **token usage**, **model details**, and any custom metrics. In Langfuse, every LLM generation span automatically logs token counts, cost in USD (if using an API with cost), the exact prompt input and model output, etc. [oai_citation:35‡langfuse.com](https://langfuse.com/integrations/native/opentelemetry#:~:text=An%20object%20detailing%20the%20token,counts%20for%20the%20generation) [oai_citation:36‡langfuse.com](https://langfuse.com/integrations/native/opentelemetry#:~:text=The%20calculated%20cost%20of%20the,generation%20in%20USD). Spans have standardized attributes for **model name**, parameters, **prompt text** and **completion text**, and usage metrics [oai_citation:37‡langfuse.com](https://langfuse.com/integrations/native/opentelemetry#:~:text=) [oai_citation:38‡langfuse.com](https://langfuse.com/integrations/native/opentelemetry#:~:text=). Similarly, tool-use spans can log the tool name, arguments, and results. Because these traces are timestamped, one can derive **latency metrics** (e.g. how long each tool call or each LLM call took) by examining the spans. In fact, one can set up queries like “show all runs where tool X took > N seconds” to identify latency outliers [oai_citation:39‡getsnippets.ai](https://www.getsnippets.ai/articles/snippets-ai-vs-langfuse-vs-opentelemetry#:~:text=The%20dashboard%20shines%20when%20things,overruns%20before%20the%20invoice%20lands). Some platforms also allow *natural language search* over traces, e.g. filtering by error messages or model content [oai_citation:40‡getsnippets.ai](https://www.getsnippets.ai/articles/snippets-ai-vs-langfuse-vs-opentelemetry#:~:text=The%20dashboard%20shines%20when%20things,overruns%20before%20the%20invoice%20lands). Overall, industry observability solutions track metrics like response times, token counts, API call counts, and error rates as first-class data.
- **OpenTelemetry and Standardization:** A trend in 2024–2025 is aligning these observability practices with **OpenTelemetry (OTel)**, an open standard for traces, metrics, and logs. OpenTelemetry’s GenAI Semantic Conventions define standard fields for LLM operations and related tool calls [oai_citation:41‡langfuse.com](https://langfuse.com/integrations/native/opentelemetry#:~:text=cover%20frameworks%20such%20as%20AutoGen%2C,Semantic%20Kernel%2C%20and%20more) [oai_citation:42‡getsnippets.ai](https://www.getsnippets.ai/articles/snippets-ai-vs-langfuse-vs-opentelemetry#:~:text=OpenTelemetry%3A%20The%20Universal%20Language%20of,Telemetry). This lets different tools interoperate. For example, an instrumented agent can emit OTel traces such that any backend (Langfuse, Datadog, Jaeger, etc.) can interpret “gen_ai.prompt” or “gen_ai.completion” fields uniformly [oai_citation:43‡langfuse.com](https://langfuse.com/integrations/native/opentelemetry#:~:text=,used%20by%20MLFlow) [oai_citation:44‡langfuse.com](https://langfuse.com/integrations/native/opentelemetry#:~:text=). Langfuse explicitly supports receiving OpenTelemetry traces and maps them to its own data model [oai_citation:45‡langfuse.com](https://langfuse.com/integrations/native/opentelemetry#:~:text=native%20integrations,AutoGen%2C%20Semantic%20Kernel%2C%20and%20more), since the community is *“pushing for standardized semantic conventions for LLM/GenAI applications”* [oai_citation:46‡langfuse.com](https://langfuse.com/blog/2024-10-opentelemetry-for-llm-observability#:~:text=The%20OpenTelemetry%20Special%20Interest%20Group,project%20doc%20and%20meeting%20notes). The benefit is that developers can use existing OTel SDKs in their agent code and not be locked to one vendor – *“OpenTelemetry standardizes traces, metrics, and logs so nothing gets trapped in proprietary formats.”* [oai_citation:47‡getsnippets.ai](https://www.getsnippets.ai/articles/snippets-ai-vs-langfuse-vs-opentelemetry#:~:text=Three%20tools%20tackle%20these%20in,production%20without%20the%20usual%20stumbles) [oai_citation:48‡getsnippets.ai](https://www.getsnippets.ai/articles/snippets-ai-vs-langfuse-vs-opentelemetry#:~:text=with%20one%20hotkey.%20,gets%20trapped%20in%20proprietary%20formats). In practice, you might use an OpenTelemetry Python SDK that auto-instruments your LLM API calls, producing spans labeled as “LLM request” with attributes for model, input, output, etc., and send those to a backend of choice [oai_citation:49‡langfuse.com](https://langfuse.com/integrations/native/opentelemetry#:~:text=Because%20it%20lives%20in%20the,Langfuse%20traces%20without%20additional%20configuration) [oai_citation:50‡getsnippets.ai](https://www.getsnippets.ai/articles/snippets-ai-vs-langfuse-vs-opentelemetry#:~:text=semantic%20conventions%20for%20LLM%20calls%2C,the%20data%20without%20custom%20parsers). This standardization is evolving (the GenAI conventions were still being refined in 2024), but tools like Langfuse already adopt them and contribute to their development [oai_citation:51‡langfuse.com](https://langfuse.com/integrations/native/opentelemetry#:~:text=As%20the%20Semantic%20Conventions%20for,not%20parse%20the%20correct%20attributes) [oai_citation:52‡langfuse.com](https://langfuse.com/blog/2024-10-opentelemetry-for-llm-observability#:~:text=Current%20State).
- **Alerts and Dashboards:** Industry observability isn’t just passive tracing; teams set up **monitoring dashboards** and **alerts** on these metrics. For example, one might track average **task success rate**, or the **hallucination rate** (if defined via an eval), over time on a dashboard. Langfuse and similar platforms allow defining **quality metrics** and watching them “climb” or drop as you experiment [oai_citation:53‡getsnippets.ai](https://www.getsnippets.ai/articles/snippets-ai-vs-langfuse-vs-opentelemetry#:~:text=Evals%20close%20the%20loop,discounts%20apply%20above%201M%20units). Some systems integrate with APM tools: for instance, Datadog’s *LLM Observability* can ingest evaluation scores (accuracy, relevancy, etc.) alongside latency and throughput metrics, enabling alerts if quality falls below a threshold [oai_citation:54‡datadoghq.com](https://www.datadoghq.com/blog/nvidia-nemo-evaluator/#:~:text=Datadog%20places%20your%20NeMo%20Evaluation,quality%20falls%20below%20a%20threshold) [oai_citation:55‡datadoghq.com](https://www.datadoghq.com/blog/nvidia-nemo-evaluator/#:~:text=data%20like%20the%20total%20latency%2C,quality%20falls%20below%20a%20threshold). An example given is attaching an “accuracy” score from an eval to each trace and then monitoring that – if the accuracy score average for the agent drops, an alert can trigger [oai_citation:56‡datadoghq.com](https://www.datadoghq.com/blog/nvidia-nemo-evaluator/#:~:text=View%20evaluation%20metrics%20in%20context). This tight integration of eval metrics with telemetry data helps connect model performance with system performance (e.g. correlating a quality drop with a spike in latency or memory usage) [oai_citation:57‡datadoghq.com](https://www.datadoghq.com/blog/nvidia-nemo-evaluator/#:~:text=Datadog%20places%20your%20NeMo%20Evaluation,quality%20falls%20below%20a%20threshold).
- **Logging Errors & Safety Events:** In observability logs, any **tool failures or exceptions** are captured as events/spans, often with severity levels. If a tool returns an error or the agent encounters a rule violation, those appear in traces (sometimes flagged as WARNING/ERROR spans with a status message) [oai_citation:58‡langfuse.com](https://langfuse.com/integrations/native/opentelemetry#:~:text=) [oai_citation:59‡langfuse.com](https://langfuse.com/integrations/native/opentelemetry#:~:text=). This is crucial for **safety monitoring** – e.g., if the agent attempted an unauthorized action, the guardrail or security module could log an event which becomes part of the trace. By querying the logs, developers can track how often safety triggers occur (e.g. “X% of conversations had a disallowed content warning”). Some platforms support **user feedback integration** as well – e.g. logging when a user gives a thumbs-down to a response – which can count towards a “quality/safety” metric for real-world interactions [oai_citation:60‡langfuse.com](https://langfuse.com/blog/2024-10-opentelemetry-for-llm-observability#:~:text=optimize%20performance%20metrics%2C%20and%20explore,new%20functionalities).

In summary, industry observability tooling focuses on *instrumenting agent systems to capture every step and outcome.* They track metrics like **latency per step, token usage, cost, error rates,** and even custom eval scores, by structuring the agent’s execution into traceable units. OpenTelemetry is embraced as the common protocol, enabling data from agent runs to flow into **central dashboards or stores** for analysis [oai_citation:61‡getsnippets.ai](https://www.getsnippets.ai/articles/snippets-ai-vs-langfuse-vs-opentelemetry#:~:text=OpenTelemetry%3A%20The%20Universal%20Language%20of,Telemetry) [oai_citation:62‡getsnippets.ai](https://www.getsnippets.ai/articles/snippets-ai-vs-langfuse-vs-opentelemetry#:~:text=The%20collector%20acts%20as%20a,FastAPI%2C%20SQLAlchemy%2C%20and%20OpenAI%20clients). This provides the backbone for evaluating agents continuously in production, complementing the one-time benchmarks from research.

## Agentic Evaluation in Practice (RAGAS, NeMo, etc.)

Beyond raw observability, vendors and open-source tools have introduced specialized **agent evaluation suites**. These aim to measure an agent’s performance on particular criteria, often leveraging LLMs for judgment. Two notable examples are **RAGAS** and NVIDIA’s **NeMo** evaluator, which extend evaluation to the agent’s use of tools and multi-step reasoning:

- **RAGAS (Retrieval-Augmented Generation Assessment):** Originally developed for evaluating RAG pipelines, RAGAS (open-source, 2023–2025) has added support for agentic workflows. It provides a library of metrics to score multi-turn interactions and tool use [oai_citation:63‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=Agentic%20or%20tool%20use%20workflows,tools%20in%20a%20given%20task) [oai_citation:64‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=ability%20of%20the%20AI%20to,queries%20related%20to%20predefined%20domains). **Metrics offered include:** *Topic Adherence*, *Tool Call Accuracy*, *Tool Call F1*, and *Agent Goal Accuracy*, among others [oai_citation:65‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=,Use%20Cases) [oai_citation:66‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=,Agent%20Goal%20Accuracy). Each metric is implemented to compare the agent’s behavior against expectations:

  - **Topic Adherence:** Measures if the agent’s responses stayed within a predefined set of topics or domains [oai_citation:67‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=Topic%20Adherence). This is important for agents with domain constraints (e.g. an IT helpdesk agent shouldn’t drift into unrelated topics). RAGAS computes precision/recall of adherence by checking if answered queries match allowed topics versus when it strayed or improperly refused queries [oai_citation:68‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=,for%20topic%20adherence%2C%20defined%20as) [oai_citation:69‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=%5C%5B%20%5Ctext,). A high topic-adherence score means the agent consistently kept the conversation within the intended scope.
  - **Tool Call Accuracy:** Evaluates whether the agent invoked the correct sequence of tools with correct arguments, compared to an expected ideal sequence [oai_citation:70‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=,step%20workflows). This requires a **ground-truth reference of tool calls** for the task. The metric gives a score 0–1, only reaching 1.0 if the agent called *all the right tools in the right order with the right parameters* [oai_citation:71‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=,step%20workflows) [oai_citation:72‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=Scoring%3A). RAGAS supports a *“strict order”* mode (default, sequence must exactly match) and a *“flexible order”* mode (if order doesn’t matter, it checks set equality of calls) [oai_citation:73‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=Two%20Evaluation%20Modes%3A) [oai_citation:74‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=2,can%20be%20in%20any%20order). It also evaluates argument correctness for each tool call, often by simple matching or string similarity unless custom comparators are provided [oai_citation:75‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=,aligned%20%3F%201%20%3A%200) [oai_citation:76‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=Partial%20argument%20match%3A). In essence, Tool Call Accuracy is a precise measure of the agent’s execution fidelity to an expected plan – e.g. did it use the `get_user_info` API when it should, or call an incorrect function.
  - **Tool Call F1:** A complementary metric that treats tool usage like a set to compute precision and recall [oai_citation:77‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=,Agent%20Goal%20Accuracy) [oai_citation:78‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=,Semantic%20Similarity). Instead of all-or-nothing scoring, Tool Call F1 gives partial credit. For example, if an agent missed one out of three required tool calls, precision/recall might reflect that. This is useful when some variance in tool sequence is acceptable and we want to reward partial correctness.
  - **Agent Goal Accuracy:** Assesses whether the agent ultimately achieved the user’s goal. RAGAS provides two variants: *With Reference* and *Without Reference* [oai_citation:79‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=,Deprecated) [oai_citation:80‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=Without%20Reference). **With Reference** means a ground-truth outcome or description of success is given, and the agent’s final output or end state is compared to that reference [oai_citation:81‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=With%20Reference) [oai_citation:82‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=user_input%20%3D%20%5B%20HumanMessage%28%20content%3D,tool_calls%3D%5B%20ToolCall). For instance, if the task is booking a meeting, the reference might be “Meeting scheduled at requested time,” and the agent’s actions/output are checked against that. This comparison may be done via semantic similarity or pattern matching on the outcome (exact method depends on implementation). **Without Reference** is used when no explicit expected outcome is provided – the metric then uses an LLM to infer the user’s intended goal from the conversation and judge if the agent’s response achieved it [oai_citation:83‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=,the%20conversation%2C%20then%20compares%20them) [oai_citation:84‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=user%27s%20goal%20without%20requiring%20a,the%20conversation%2C%20then%20compares%20them). Under the hood, this effectively becomes an LLM-as-judge evaluating success. Agent Goal Accuracy is typically binary (1 or 0 for each scenario) and averaged over many runs [oai_citation:85‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=Agent%20goal%20accuracy%20is%20a,has%20not%20achieved%20the%20goal). It directly measures *task-level success* from the user’s perspective, irrespective of how many steps it took.

  These metrics allow evaluation of both **process** (tool usage, topic focus) and **outcome** (goal success). RAGAS can be run on logged traces after an agent performs, or in a testing loop. Notably, RAGAS makes heavy use of *LLM-based evaluation*: Topic Adherence and AgentGoalAccuracyWithoutReference rely on an LLM to interpret content and judge adherence or success, since no simple ground truth string exists [oai_citation:86‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=,the%20conversation%2C%20then%20compares%20them) [oai_citation:87‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=metric%20%3D%20AgentGoalAccuracyWithoutReference,Agent%20Goal%20Accuracy%3A%20%7Bresult.value). Other metrics like Tool Call Accuracy use explicit references (ground truth sequences) and do straightforward comparisons not requiring an LLM (hence these can be automated fully) [oai_citation:88‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=,step%20workflows) [oai_citation:89‡docs.nvidia.com](https://docs.nvidia.com/nemo/microservices/latest/evaluate/flows/agentic.html#:~:text=Model%20Configuration%3A).

- **NVIDIA NeMo Agentic Evaluator:** NVIDIA’s NeMo platform (Microservices 25.x) includes an *Agent Evaluator* service that systematically evaluates agent behavior. It breaks evaluation into **stages and types** [oai_citation:90‡docs.nvidia.com](https://docs.nvidia.com/nemo/microservices/latest/evaluate/flows/agentic.html#:~:text=Agentic%20evaluation%20flow%20assesses%20the,tool%20use%2C%20and%20iterative%20reasoning) [oai_citation:91‡docs.nvidia.com](https://docs.nvidia.com/nemo/microservices/latest/evaluate/flows/agentic.html#:~:text=2.%20Final,the%20agent%E2%80%99s%20final%20output%20using), aligning with how an agent operates:
  1. **Intermediate Steps Evaluation:** Check each step’s correctness during the agent’s reasoning. A key metric here is **Tool Use Accuracy** – NeMo validates that *“the agent invoked the right tools with correct arguments at each step.”* [oai_citation:92‡docs.nvidia.com](https://docs.nvidia.com/nemo/microservices/latest/evaluate/flows/agentic.html#:~:text=1,intermediate%20steps%20during%20agent%20execution) This is analogous to RAGAS’s Tool Call Accuracy. If the agent was supposed to call certain tools in sequence, this eval will flag any deviations or mistakes in tool arguments [oai_citation:93‡docs.nvidia.com](https://docs.nvidia.com/nemo/microservices/latest/evaluate/flows/agentic.html#:~:text=1,intermediate%20steps%20during%20agent%20execution) [oai_citation:94‡docs.nvidia.com](https://docs.nvidia.com/nemo/microservices/latest/evaluate/flows/agentic.html#:~:text=,Call%20Accuracy%20for%20implementation%20details). Another intermediate metric is **Retriever Quality** (for RAG agents) – if the agent uses a retrieval step, NeMo can evaluate whether relevant documents were fetched successfully [oai_citation:95‡docs.nvidia.com](https://docs.nvidia.com/nemo/microservices/latest/evaluate/flows/agentic.html#:~:text=arguments%20at%20each%20step,Call%20Accuracy%20for%20implementation%20details) (often using information-retrieval metrics or a judge LLM to see if the retrieved context was useful).
  2. **Final Step Evaluation:** Assess the final output’s quality. Here NeMo offers **Agent Goal Accuracy** (similar to above, did the agent fulfill the request), which it can do *with a reference or without a reference* depending on if there’s a ground-truth answer [oai_citation:96‡docs.nvidia.com](https://docs.nvidia.com/nemo/microservices/latest/evaluate/flows/agentic.html#:~:text=using%3A). It also provides **Topic Adherence** evaluation to ensure the agent stayed on topic or within scope throughout the conversation [oai_citation:97‡docs.nvidia.com](https://docs.nvidia.com/nemo/microservices/latest/evaluate/flows/agentic.html#:~:text=Accuracy%20without%20Reference%20). Additionally, NeMo supports **Answer Correctness** for factual Q&A tasks (comparing the answer to a known correct answer) [oai_citation:98‡docs.nvidia.com](https://docs.nvidia.com/nemo/microservices/latest/evaluate/flows/agentic.html#:~:text=Agent%20Goal%20Accuracy%20with%20Reference) [oai_citation:99‡docs.nvidia.com](https://docs.nvidia.com/nemo/microservices/latest/evaluate/flows/agentic.html#:~:text=Answer%20Accuracy). For more subjective or custom success criteria, NeMo integrates **LLM-as-a-Judge** evaluation: you can supply a prompt-based rubric and have an LLM score the agent’s final output on custom metrics (like helpfulness, style, etc.) [oai_citation:100‡docs.nvidia.com](https://docs.nvidia.com/nemo/microservices/latest/evaluate/flows/agentic.html#:~:text=assigned%20topic%20throughout%20the%20conversation,See%20Topic%20Adherence). In fact, NeMo’s evaluator expects you to configure a *“Judge LLM”* for most agentic eval tasks, which will be used to grade responses against the criteria – *“required for most agentic tasks – Tool Call Accuracy is the exception”* [oai_citation:101‡docs.nvidia.com](https://docs.nvidia.com/nemo/microservices/latest/evaluate/flows/agentic.html#:~:text=Model%20Configuration%3A). This indicates that NeMo relies on an LLM to assess things like goal completion (without reference) or topic adherence, whereas tool accuracy can be computed automatically from structured logs.
  3. **Trajectory Evaluation:** This is an assessment of the agent’s overall **decision-making sequence** [oai_citation:102‡docs.nvidia.com](https://docs.nvidia.com/nemo/microservices/latest/evaluate/flows/agentic.html#:~:text=3,Trajectory%20Evaluation%20for%20configuration%20details). Rather than judging steps in isolation, trajectory evaluation looks at whether the *entire sequence of actions was appropriate* to reach the goal [oai_citation:103‡docs.nvidia.com](https://docs.nvidia.com/nemo/microservices/latest/evaluate/flows/agentic.html#:~:text=3,Trajectory%20Evaluation%20for%20configuration%20details). For example, it examines if the agent’s plan was efficient and logically sound: Did it choose an optimal order of tool calls? Did it go in circles or take redundant steps? NeMo’s config allows analysis of the chain-of-thought and the sequence of tool invocations to rate the trajectory [oai_citation:104‡docs.nvidia.com](https://docs.nvidia.com/nemo/microservices/latest/evaluate/flows/agentic.html#:~:text=3,Trajectory%20Evaluation%20for%20configuration%20details). This is a more holistic measure – a trajectory might be marked down if the agent eventually succeeded but took an unnecessarily convoluted path.

  NeMo’s evaluations output *structured scores per criterion* (often on 0–1 or 0–100 scales). These can include traditional metrics (it supports **accuracy, ROUGE, BLEU** for certain tasks) as well as learned ones via LLM judging [oai_citation:105‡datadoghq.com](https://www.datadoghq.com/blog/nvidia-nemo-evaluator/#:~:text=reasoning%2C%20coding%2C%20retrieval%2C%20and%20instruction). A big advantage is integration: it is built to plug into pipelines and even into monitoring tools. As one blog notes, *“NVIDIA NeMo Evaluator returns structured scores for each model response”* and these can be automatically logged and visualized [oai_citation:106‡datadoghq.com](https://www.datadoghq.com/blog/nvidia-nemo-evaluator/#:~:text=With%20NVIDIA%20NeMo%20Evaluator%2C%20developers,data%20flywheels%20for%20continuous%20evaluation) [oai_citation:107‡datadoghq.com](https://www.datadoghq.com/blog/nvidia-nemo-evaluator/#:~:text=Collect%20NeMo%20Evaluator%20scores%20in,LLM%20Observability). For instance, using Datadog’s integration, teams tie NeMo’s eval scores (e.g. an accuracy or a “helpfulness” score from a judge LLM) into the trace of the original request, so they can see quality metrics alongside latency and other telemetry [oai_citation:108‡datadoghq.com](https://www.datadoghq.com/blog/nvidia-nemo-evaluator/#:~:text=Datadog%20LLM%20Observability%20provides%20end,complete%20view%20of%20each%20request) [oai_citation:109‡datadoghq.com](https://www.datadoghq.com/blog/nvidia-nemo-evaluator/#:~:text=Datadog%20places%20your%20NeMo%20Evaluation,quality%20falls%20below%20a%20threshold).

**Other vendor examples:** OpenAI’s **Evals** framework (2023) is another tool that, while not specifically agent-focused, can be adapted to test multi-step agent scenarios with both static checks and LLM-based grading. There are also emerging platforms (like LlamaIndex’s eval module, LangChain’s LangSmith eval) that let you generate test conversations for an agent and use an LLM to score correctness or compliance. The **common thread** in practice is combining **ground-truth checks** when possible (for objective facts or expected tool/API outcomes) with **LLM-as-judge evaluations** for subjective or hard-to-formalize criteria. This hybrid approach is seen in RAGAS (which offers both reference-based and reference-free metrics) and NeMo (which mixes factual metrics with LLM “judges”). It provides coverage of both **quantitative metrics** (latency, success rate, counts) and **qualitative judgments** (e.g. “Was the explanation clear and polite?” as judged by GPT-4).

## LLM-as-Judge vs Ground-Truth Evaluation

A key question in evaluating agent outputs is whether to rely on **ground truth references** or to use an **LLM as a surrogate judge** of quality. Recent work explores both, often in complementary ways:

- **Ground-Truth (Reference-Based) Evaluation:** This method uses a predefined correct answer or outcome to compare against the agent’s result. It’s the traditional approach for tasks where a single right answer or a set of valid answers is known. For instance, if an agent’s task is to sort a list or retrieve a specific piece of data, you can directly check its output against the correct solution. Metrics like exact match accuracy, BLEU/ROUGE (for textual similarity to a reference), or programmatic checks (did the database entry actually get created?) fall in this category [oai_citation:110‡datadoghq.com](https://www.datadoghq.com/blog/nvidia-nemo-evaluator/#:~:text=With%20NVIDIA%20NeMo%20Evaluator%2C%20developers,data%20flywheels%20for%20continuous%20evaluation). Ground-truth evaluation is **objective and reproducible**, but it requires having an expected result for each query – which is feasible for deterministic tasks (math problems, factual QA with a known answer, etc.) but **difficult for open-ended tasks**. Many agent scenarios (e.g. “have a helpful conversation about my IT issue”) don’t have a single correct answer. Furthermore, ground-truth metrics can be too rigid: an agent answer might be correct but phrased very differently from the reference, leading naive string-based metrics to score it low [oai_citation:111‡qualifire.ai](https://www.qualifire.ai/posts/llm-evaluation-frameworks-metrics-methods-explained#:~:text=generated%20text%20to%20reference%20texts%2C,answer%20or%20reference%20output%20exists). Hence, ground-truth eval is often used for *narrow sub-tasks* of agents (like verifying if a tool’s output was used correctly, or checking a factual answer) and in benchmarks where each task is well-specified. For example, RAGAS’s ToolCallAccuracy and AgentGoalAccuracyWithReference both require ground-truth references (expected tool sequence, expected outcome) and thereby provide clear yes/no or numeric comparisons [oai_citation:112‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=The%20metric%20requires%20,higher%20values%20indicate%20better%20performance) [oai_citation:113‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=With%20Reference). When available, reference-based evaluation is very reliable – e.g. checking if “file X was actually created” by the agent can be done via unit tests on the environment state, removing ambiguity.

- **LLM-as-Judge (AI-based Evaluation):** Here, one uses a strong LLM (often GPT-4 or similar) to evaluate the agent’s performance by *prompting it with the interaction and asking for a judgment*. This approach has gained popularity because it handles nuanced evaluation without explicit references [oai_citation:114‡qualifire.ai](https://www.qualifire.ai/posts/llm-evaluation-frameworks-metrics-methods-explained#:~:text=Hybrid%20evaluation%20methods%20leverage%20AI,and%20part%20of%20the%20solution) [oai_citation:115‡qualifire.ai](https://www.qualifire.ai/posts/llm-evaluation-frameworks-metrics-methods-explained#:~:text=LLM,chatbot%20helpfulness%20to%20code%20correctness). Essentially, the LLM can be asked questions like: *“Given the user’s request and the agent’s response, did the agent fulfill the request correctly and helpfully? Give a score and justification.”* The LLM, as a “judge”, can consider context, language quality, and even policy compliance if instructed. **G-Eval** (Liu et al., 2023) is a well-known framework that formalized this: it uses GPT-4 with a chain-of-thought prompt to score responses on multiple criteria [oai_citation:116‡qualifire.ai](https://www.qualifire.ai/posts/llm-evaluation-frameworks-metrics-methods-explained#:~:text=What%20is%20G,It%20Matters) [oai_citation:117‡qualifire.ai](https://www.qualifire.ai/posts/llm-evaluation-frameworks-metrics-methods-explained#:~:text=G,Natural). LLM-as-judge is very flexible – it can evaluate virtually any aspect (from factual accuracy to tone or safety) by appropriately crafting the prompt criteria [oai_citation:118‡qualifire.ai](https://www.qualifire.ai/posts/llm-evaluation-frameworks-metrics-methods-explained#:~:text=Strengths%20of%20G) [oai_citation:119‡qualifire.ai](https://www.qualifire.ai/posts/llm-evaluation-frameworks-metrics-methods-explained#:~:text=,4). It also doesn’t require a target answer, which is ideal for open-ended or creative tasks. Many frameworks mentioned earlier leverage this: e.g. NeMo’s “Without Reference” metrics and custom criteria rely on a “judge LLM” that has been configured to follow a rubric [oai_citation:120‡docs.nvidia.com](https://docs.nvidia.com/nemo/microservices/latest/evaluate/flows/agentic.html#:~:text=assigned%20topic%20throughout%20the%20conversation,See%20Topic%20Adherence) [oai_citation:121‡docs.nvidia.com](https://docs.nvidia.com/nemo/microservices/latest/evaluate/flows/agentic.html#:~:text=Model%20Configuration%3A). RAGAS similarly uses an internal LLM to infer goal success when no reference is provided [oai_citation:122‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=Without%20Reference) [oai_citation:123‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=,the%20conversation%2C%20then%20compares%20them). In practice, LLM-as-judge has been found to correlate well with human judgments in many cases [oai_citation:124‡qualifire.ai](https://www.qualifire.ai/posts/llm-evaluation-frameworks-metrics-methods-explained#:~:text=match%20at%20L355%20,4), and it’s **much faster and scalable** than having humans rate each result. However, it comes with caveats: the LLM judge can be biased (e.g. favoring more verbose answers), non-deterministic (might give slightly different scores each run), and it can make mistakes or be tricked by clever outputs. Prompting techniques like chain-of-thought and asking for detailed justification (as in G-Eval) help improve judge consistency [oai_citation:125‡qualifire.ai](https://www.qualifire.ai/posts/llm-evaluation-frameworks-metrics-methods-explained#:~:text=G,this%20regard) [oai_citation:126‡qualifire.ai](https://www.qualifire.ai/posts/llm-evaluation-frameworks-metrics-methods-explained#:~:text=match%20at%20L306%20G,criterion%20explaining%20how%20the%20output), but some variability remains [oai_citation:127‡qualifire.ai](https://www.qualifire.ai/posts/llm-evaluation-frameworks-metrics-methods-explained#:~:text=%2A%20Non,as%20evaluators%20is%20that%20their). Therefore, teams often **combine AI judging with occasional human spot-checks**, or statistical analysis to ensure the AI eval is stable on average [oai_citation:128‡qualifire.ai](https://www.qualifire.ai/posts/llm-evaluation-frameworks-metrics-methods-explained#:~:text=%2A%20Non,as%20evaluators%20is%20that%20their) [oai_citation:129‡qualifire.ai](https://www.qualifire.ai/posts/llm-evaluation-frameworks-metrics-methods-explained#:~:text=judgments%20can%20vary%20slightly%20from,For%20consistent).

In summary, **ground-truth evaluation** is used whenever you have a concrete expected outcome or can directly measure success (e.g. did the agent book the table as instructed?), while **LLM-based evaluation** fills the gaps by assessing quality in a more subjective or complex sense (e.g. was the explanation clear and aligned with policy). Modern agent evaluation frameworks blend both: for example, check hard correctness with references where possible, and use an LLM to judge properties like “Was the tool usage justified?” or “Is the final answer satisfactory?” where no explicit answer key exists. Notably, reliance on LLM-as-judge is rising, but careful prompt design and verification are important to avoid the AI evaluator’s biases propagating into the metrics [oai_citation:130‡qualifire.ai](https://www.qualifire.ai/posts/llm-evaluation-frameworks-metrics-methods-explained#:~:text=match%20at%20L383%20No%20method,judge%20can%20falter) [oai_citation:131‡qualifire.ai](https://www.qualifire.ai/posts/llm-evaluation-frameworks-metrics-methods-explained#:~:text=%2A%20Non,as%20evaluators%20is%20that%20their).

## Recommended Evaluation Schema for a Local IT Assistant Agent

Finally, combining these insights, we propose an evaluation and observability schema for a local IT assistant agent. This agent might handle tasks like answering IT support questions, running diagnostics via tools, and following company policies. The evaluation schema should cover **task-level success, tool usage, and safety**, with integration into an OpenTelemetry-based observability stack for real-time monitoring. Key components of the schema:

- **Task-Level Metrics:** These assess each user request at the overall outcome level. For a given IT query or command, did the agent ultimately fulfill it correctly and efficiently?
  - **Success Rate:** Define what success means for various tasks (e.g. problem resolved, correct answer given, ticket filed in system, etc.). Track the fraction of queries that the agent handles successfully without human intervention. Each session can be labeled success or failure (or partial success), either via an automated check (if the outcome can be verified in the IT system) or via an LLM judge/human feedback. For instance, if a user asks to reset a password, success could be verified by seeing that the password reset actually occurred in the system, or by the agent’s final message confirming it and the user not asking again. This metric gives a top-line measure of agent effectiveness.
  - **Task Completion Time / Turns:** Measure how long (in wall-clock time and in number of dialogue turns) it takes for the agent to complete the task. A good IT assistant should be efficient. Using trace timestamps, record the latency from user’s initial query to final resolution. Also count dialogue turns or steps taken. This helps identify if the agent is getting stuck in long back-and-forth or taking too long due to slow tools. You can set thresholds (e.g. flag tasks that took >N seconds or >M tool calls).
  - **Output Quality and User Satisfaction:** Although hard to quantify automatically, it’s important to evaluate the clarity and helpfulness of the agent’s final response. This can be done by soliciting user feedback (star ratings or “Was this answer helpful? [Yes/No]”) and/or by an **LLM-as-judge** that scores the final answer on a rubric (correctness, clarity, tone). For example, periodically run an offline evaluation where GPT-4 is given the conversation and asked: “Rate if the assistant’s answer resolved the issue and was easy to understand (1-5).” Monitor these quality scores over time. A high task success with low satisfaction would indicate the agent solves the problem but perhaps communicates poorly or verbosely. Both objective success and subjective quality should be tracked [oai_citation:132‡arxiv.org](https://arxiv.org/html/2507.21504v1#:~:text=Output%20Quality%20Coherence%2C%20User%20Satisfaction%2C,Shi%20et%C2%A0al).
  - **Escalation/Failure Rate:** Since this is a local assistant, note how often it has to escalate to a human or fails to complete the task. Every time the agent says “I cannot help with that” or triggers a fallback to a human operator, log this as an event. Over time, you want to reduce this rate, but also monitor it to ensure the agent *is* correctly deferring when out of its depth (rather than attempting something unsafe).

- **Tool-Call and Process Metrics:** These drill down into how the agent uses its tools and what happens during each step:
  - **Tool Call Accuracy & Errors:** For each tool/API the agent can use (e.g. an Active Directory lookup, a software deployment script, a knowledge base search), track if the calls were *successful* and *correct*. Using instrumentation around each tool invocation, log whether the call succeeded (e.g. API returned 200 OK, or script executed without error) and whether the results were used appropriately. If you have an expected sequence for certain workflows, you can implement checks – e.g. for a “reset password” task, the expected tool sequence might be `verify_user` then `reset_password`. If the agent deviates, flag that. Even without strict sequences, you can calculate **ToolCallAccuracy** per session by comparing to a reference or at least measuring if any necessary tool was missed. Additionally, compute a **Tool Call Success Rate** = (number of tool calls that returned a valid result) / (total tool calls). This catches issues like the agent calling a command with wrong arguments (leading to an error).
  - **Tool Utilization Count:** Monitor how many tool calls the agent makes per task and which tools are used most. An efficient agent shouldn’t call 10 tools when 2 would do; excessive calls may indicate looping or uncertainty. You might set an alert if the number of tool invocations in a single session exceeds a plausible limit. Also track distribution: e.g. 60% of sessions used the “search KB” tool, 30% used “reset password” tool, etc., to understand usage patterns and where optimizations might be needed.
  - **Sequence & Planning Quality:** To gauge the agent’s planning, you can introduce a metric for **optimal action sequence**. This could be manual or learned: define an ideal sequence of actions for common tasks and then measure edit distance between the agent’s sequence and the ideal. Alternatively, use an LLM judge: e.g. prompt “Did the agent use a reasonable sequence of steps to solve the problem?” after the conversation, to score the planning aspect (this aligns with NeMo’s trajectory evaluation [oai_citation:133‡docs.nvidia.com](https://docs.nvidia.com/nemo/microservices/latest/evaluate/flows/agentic.html#:~:text=3,Trajectory%20Evaluation%20for%20configuration%20details)). This metric helps ensure the agent isn’t doing extraneous steps like needless tool calls or redundant questions.
  - **Latency per Step:** For each tool call and each LLM call, record latency (already captured via tracing). From this, derive metrics like **average tool latency** (by tool type), and highlight any tool or step that is a bottleneck. If the local assistant runs on-premise tools, one slow database query could drag performance – having per-span latency metrics lets you pinpoint that (e.g. “KB search took 5s on average, needs caching”).

- **Safety and Supervision Metrics:** Since this is an IT assistant likely integrated with internal systems, safety and compliance are paramount:
  - **Policy Violation Count:** Instrument the agent’s output filter or guardrail system to log any time the agent’s response is modified or stopped due to a policy (for example, if it tried to reveal sensitive info or perform an unauthorized action). Count how often this happens. Ideally it should be zero in normal use; a non-zero count indicates either users are prompting disallowed requests or the agent policy needs refinement.
  - **Security Audit Events:** If the agent has access to IT infrastructure, ensure that all actions are logged with audit trails. For evaluation, treat any unauthorized attempt (agent tries to access a resource it shouldn’t) as a critical event. Track these events over time. Even attempted violations (caught by security checks) are important to monitor – it could indicate the agent misunderstood instructions or was prompted maliciously.
  - **User Override / Feedback:** In cases where the agent gives an answer but the user disagrees or marks it unhelpful, log that as a *safety/supervision signal* too. For instance, if the agent provided a potentially harmful instruction (e.g. a command that could damage a system) and the user or a human supervisor had to intervene, record it. One concrete metric: number of **manual corrections** or **human interventions** per 100 sessions. In an IT context, this might be escalation to a live support or the user rephrasing with frustration. Each instance should ideally be reviewed to classify if it was a safety issue, a capability gap, or a misunderstanding.
  - **Content Filter Flags:** If using a content moderation model on the agent’s outputs, count how often outputs are flagged (toxic, sensitive, etc.). The IT assistant hopefully won’t produce much disallowed content, but it might inadvertently reveal private info. By tracking flags, you can quantify how well the agent stays within safe bounds. E.g. “agent attempted to output a user’s password in 2 cases (flagged and blocked)” – that’s a serious policy issue to fix.

- **OpenTelemetry Traces & Dashboard Integration:** All the above metrics should be collected through a unified observability pipeline. The **agent will be instrumented with OpenTelemetry** to emit traces and metrics:
  - **Tracing:** Each user session (conversation) is a trace. Spans within it include: “User query received”, “Agent LLM thought chain step 1”, “Tool X call”, “Tool X result”, “Agent LLM thought step 2”, ..., “Final answer to user”. Use the emerging OTel semantic attributes for GenAI where possible (e.g. mark the LLM call spans with `gen_ai.model`, `gen_ai.prompt`, etc., and tool spans with names like `app.tool.name=...`). Include custom attributes for critical pieces: tool name, arguments, return values (if not sensitive), and any error codes. Also propagate a **session ID** or user ID if needed for correlation [oai_citation:134‡langfuse.com](https://langfuse.com/integrations/native/opentelemetry#:~:text=,metadata) [oai_citation:135‡langfuse.com](https://langfuse.com/integrations/native/opentelemetry#:~:text=). The trace will thus capture the full timeline and context, which you can visualize in a tool like Jaeger or in Langfuse’s UI if you send traces there.
  - **Metrics Collection:** Use OpenTelemetry metrics (or a sidecar) to collect the quantitative metrics mentioned. For example, you can have a counter for “tasks_completed” (tagged by success or fail), a histogram for “task_completion_time”, a counter for each tool usage (tagged by tool name and outcome), and a gauge or counter for “policy_violations”. Many of these can be derived from spans (OTel allows turning span data into metrics via aggregations). For instance, every span representing a tool call could update a Prometheus counter `{tool_name, status}`. The evaluation scores (like an LLM judge’s rating for that session) can be reported as metrics too – e.g. a histogram for “quality_score” or simply log it as a span attribute that’s scraped. In the Datadog example, they attach NeMo’s scores to spans and then visualize them [oai_citation:136‡datadoghq.com](https://www.datadoghq.com/blog/nvidia-nemo-evaluator/#:~:text=Datadog%20LLM%20Observability%20provides%20end,complete%20view%20of%20each%20request) [oai_citation:137‡datadoghq.com](https://www.datadoghq.com/blog/nvidia-nemo-evaluator/#:~:text=Datadog%20places%20your%20NeMo%20Evaluation,quality%20falls%20below%20a%20threshold); you can do similarly by adding a custom attribute like `evaluation.score` to the final answer span, and exporting that.
  - **Local Dashboard:** Set up a local **metrics store and dashboard** – for an open solution, one might use Prometheus for metrics and Grafana for visualization, plus Jaeger or Grafana Tempo for trace viewing. The OpenTelemetry Collector can route trace data to Jaeger (for examining individual traces in detail) and metrics data to Prometheus. Grafana can then read Prometheus to display trends (graphs for success rate, latency, etc.) and even integrate with Jaeger to link from a high-level metric to example traces. If using Langfuse self-hosted (an option since Langfuse offers Docker deployment [oai_citation:138‡getsnippets.ai](https://www.getsnippets.ai/articles/snippets-ai-vs-langfuse-vs-opentelemetry#:~:text=per%20language%29%20Self,agnostic%20signals)), Langfuse itself provides a dashboard where you can search and view traces, and it has built-in evaluation and comparison tooling [oai_citation:139‡getsnippets.ai](https://www.getsnippets.ai/articles/snippets-ai-vs-langfuse-vs-opentelemetry#:~:text=The%20dashboard%20shines%20when%20things,overruns%20before%20the%20invoice%20lands) [oai_citation:140‡getsnippets.ai](https://www.getsnippets.ai/articles/snippets-ai-vs-langfuse-vs-opentelemetry#:~:text=Evals%20close%20the%20loop,discounts%20apply%20above%201M%20units). The key is to have **real-time visibility**: you should be able to see, for example, that in the last hour 95% of tasks succeeded, average resolution time was 30s, there were 2 safety rule triggers, and one particular tool had an elevated error rate.
  - **Alerts and Analysis:** Finally, configure alerts on the important metrics. For a local IT assistant, you might set alerts like: *Task success rate drops below X%* (could indicate a regression in the model or tool outage), *Median latency rises above Y seconds* (performance issue), or *Any security violation event occurs* (trigger immediate review). Also use the collected data for continuous improvement: analyze failed sessions via their traces to see why the agent failed (missed a step? misunderstood user?), and feed those cases back into training or prompt engineering. Over time, incorporate a feedback loop where frequent failure modes are addressed and the metrics dashboard reflects the improvements (e.g. success rate going up, tool errors going down, etc.).

By implementing this schema, you ensure that the **local IT assistant agent is rigorously evaluated and observable** at all levels. You’ll have **task-level insight** into how well it’s helping users, **tool-level visibility** into what it’s doing under the hood (and how well it’s doing it), and **safety oversight** to catch any missteps. Moreover, by using OpenTelemetry and a local metrics stack, you avoid vendor lock-in and keep all data on-premise (important for enterprise IT scenarios) [oai_citation:141‡getsnippets.ai](https://www.getsnippets.ai/articles/snippets-ai-vs-langfuse-vs-opentelemetry#:~:text=with%20one%20hotkey.%20,gets%20trapped%20in%20proprietary%20formats). This setup mirrors the best practices from recent research and industry efforts – combining multi-faceted evaluation metrics [oai_citation:142‡arxiv.org](https://arxiv.org/html/2507.21504v1#:~:text=evaluating%20these%20agents%20remains%20a,the%20future%20research%20directions%2C%20including) [oai_citation:143‡getmaxim.ai](https://www.getmaxim.ai/blog/llm-agent-evaluation-framework-comparison/#:~:text=Multi) with robust tracing and logging to not only measure performance but also explain and improve the agent’s behavior over time.

**Sources:** Recent literature and industry documentation have informed these recommendations, including academic surveys on LLM agent evaluation [oai_citation:144‡arxiv.org](https://arxiv.org/html/2507.21504v1#:~:text=evaluating%20these%20agents%20remains%20a,the%20future%20research%20directions%2C%20including) [oai_citation:145‡getmaxim.ai](https://www.getmaxim.ai/blog/llm-agent-evaluation-framework-comparison/#:~:text=Multi), open-source evaluation frameworks (RAGAS) [oai_citation:146‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=,step%20workflows) [oai_citation:147‡docs.ragas.io](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#:~:text=,the%20conversation%2C%20then%20compares%20them), vendor solutions like NVIDIA’s NeMo evaluator [oai_citation:148‡docs.nvidia.com](https://docs.nvidia.com/nemo/microservices/latest/evaluate/flows/agentic.html#:~:text=using%3A) [oai_citation:149‡docs.nvidia.com](https://docs.nvidia.com/nemo/microservices/latest/evaluate/flows/agentic.html#:~:text=Model%20Configuration%3A), and observability tools integrating OpenTelemetry for AI applications [oai_citation:150‡getsnippets.ai](https://www.getsnippets.ai/articles/snippets-ai-vs-langfuse-vs-opentelemetry#:~:text=Langfuse%20treats%20every%20LLM%20interaction,each%20as%20a%20clickable%20span) [oai_citation:151‡getsnippets.ai](https://www.getsnippets.ai/articles/snippets-ai-vs-langfuse-vs-opentelemetry#:~:text=OpenTelemetry%3A%20The%20Universal%20Language%20of,Telemetry). Each offers pieces of the puzzle – by integrating their approaches, a comprehensive evaluation schema can be achieved.
